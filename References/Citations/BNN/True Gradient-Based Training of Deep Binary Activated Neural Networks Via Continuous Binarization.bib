@INPROCEEDINGS{8461456,  author={C. Sakr and J. Choi and Z. Wang and K. Gopalakrishnan and N. Shanbhag}, booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},  title={True Gradient-Based Training of Deep Binary Activated Neural Networks Via Continuous Binarization},  year={2018}, volume={}, number={}, pages={2346-2350}, abstract={With the ever growing popularity of deep learning, the tremendous complexity of deep neural networks is becoming problematic when one considers inference on resource constrained platforms. Binary networks have emerged as a potential solution, however, they exhibit a fundamentallimi-tation in realizing gradient-based learning as their activations are non-differentiable. Current work has so far relied on approximating gradients in order to use the back-propagation algorithm via the straight through estimator (STE). Such approximations harm the quality of the training procedure causing a noticeable gap in accuracy between binary neural networks and their full precision baselines. We present a novel method to train binary activated neural networks using true gradient-based learning. Our idea is motivated by the similarities between clipping and binary activation functions. We show that our method has minimal accuracy degradation with respect to the full precision baseline. Finally, we test our method on three benchmarking datasets: MNIST, CIFAR-10, and SVHN. For each benchmark, we show that continuous binarization using true gradient-based learning achieves an accuracy within 1.5% of the floating-point baseline, as compared to accuracy drops as high as 6% when training the same binary activated network using the STE.}, keywords={gradient methods;learning (artificial intelligence);neural nets;gradient-based training;deep binary activated neural networks;continuous binarization;deep learning;tremendous complexity;resource constrained platforms;training procedure;binary activation functions;minimal accuracy degradation;gradient-based learning;back-propagation algorithm;straight through estimator;floating-point baseline;STE;Training;Neural networks;Complexity theory;Machine learning;Stochastic processes;Perturbation methods;Approximation algorithms;deep learning;binary neural networks;activation functions}, doi={10.1109/ICASSP.2018.8461456}, ISSN={2379-190X}, month={April},}
