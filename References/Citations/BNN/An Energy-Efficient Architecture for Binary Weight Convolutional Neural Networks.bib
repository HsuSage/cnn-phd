@ARTICLE{8103902,  author={Y. Wang and J. Lin and Z. Wang}, journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},  title={An Energy-Efficient Architecture for Binary Weight Convolutional Neural Networks},  year={2018}, volume={26}, number={2}, pages={280-293}, abstract={Binary weight convolutional neural networks (BCNNs) can achieve near state-of-the-art classification accuracy and have far less computation complexity compared with traditional CNNs using high-precision weights. Due to their binary weights, BCNNs are well suited for vision-based Internet-of-Things systems being sensitive to power consumption. BCNNs make it possible to achieve very high throughput with moderate power dissipation. In this paper, an energy-efficient architecture for BCNNs is proposed. It fully exploits the binary weights and other hardware-friendly characteristics of BCNNs. A judicious processing schedule is proposed so that off-chip I/O access is minimized and activations are maximally reused. To significantly reduce the critical path delay, we introduce optimized compressor trees and approximate binary multipliers with two novel compensation schemes. The latter is able to save significant hardware resource, and almost no computation accuracy is compromised. Taking advantage of error resiliency of BCNNs, an innovative approximate adder is developed, which significantly reduces the silicon area and data path delay. Thorough error analysis and extensive experimental results on several data sets show that the approximate adders in the data path cause negligible accuracy loss. Moreover, algorithmic transformations for certain layers of BCNNs and a memory-efficient quantization scheme are incorporated to further reduce the energy cost and on-chip storage requirement. Finally, the proposed BCNN hardware architecture is implemented with the SMIC 130-nm technology. The postlayout results demonstrate that our design can achieve an energy efficiency over 2.0TOp/s/W when scaled to 65 nm, which is more than two times better than the prior art.}, keywords={adders;convolution;energy conservation;error analysis;feedforward neural nets;multiplying circuits;neural chips;trees (mathematics);energy-efficient architecture;binary weight convolutional neural networks;high-precision weights;binary weights;approximate binary multipliers;BCNN hardware architecture;energy efficiency;classification accuracy;data path delay;processing schedule;off-chip I/O access;critical path delay;optimized compressor trees;approximate adder;error analysis;memory-efficient quantization;on-chip storage requirement;size 65.0 nm;Computer architecture;Hardware;Neural networks;Neurons;Adders;Quantization (signal);Convolution;Approximate computing;binary weight convolutional neural network (BCNN) architecture;convolutional neural network (CNN);deep learning;energy-efficient design;signal processing;VLSI architecture}, doi={10.1109/TVLSI.2017.2767624}, ISSN={1063-8210}, month={Feb},}
