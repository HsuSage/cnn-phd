@INPROCEEDINGS{8425178,  author={Y. Hu and J. Zhai and D. Li and Y. Gong and Y. Zhu and W. Liu and L. Su and J. Jin}, booktitle={2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},  title={BitFlow: Exploiting Vector Parallelism for Binary Neural Networks on CPU},  year={2018}, volume={}, number={}, pages={244-253}, abstract={Deep learning has revolutionized computer vision and other fields since its big bang in 2012. However, it is challenging to deploy Deep Neural Networks (DNNs) into real-world applications due to their high computational complexity. Binary Neural Networks (BNNs) dramatically reduce computational complexity by replacing most arithmetic operations with bitwise operations. Existing implementations of BNNs have been focusing on GPU or FPGA, and using the conventional image-to-column method that doesn't perform well for binary convolution due to low arithmetic intensity and unfriendly pattern for bitwise operations. We propose BitFlow, a gemm-operator-network three-level optimization framework for fully exploiting the computing power of BNNs on CPU. BitFlow features a new class of algorithm named PressedConv for efficient binary convolution using locality-aware layout and vector parallelism. We evaluate BitFlow with the VGG network. On a single core of Intel Xeon Phi, BitFlow obtains 1.8x speedup over unoptimized BNN implementations, and 11.5x speedup over counterpart full-precision DNNs. Over 64 cores, BitFlow enables BNNs to run 1.1x faster than counterpart full-precision DNNs on GPU (GTX 1080).}, keywords={computational complexity;field programmable gate arrays;graphics processing units;learning (artificial intelligence);multiprocessing systems;neural nets;optimisation;parallel processing;vector parallelism;binary Neural Networks;CPU;Deep learning;computer vision;big bang;Deep Neural Networks;high computational complexity;Binary Neural Networks;BNNs;arithmetic operations;bitwise operations;image-to-column method;low arithmetic intensity;gemm-operator-network;computing power;BitFlow features;efficient binary convolution;VGG network;counterpart full-precision DNNs;GPU;Convolution;Neural networks;Layout;Parallel processing;Acceleration;Graphics processing units;Machine learning;Network Compression;Binary Neural Network (BNN);Vector Parallelism;Intel Xeon Phi}, doi={10.1109/IPDPS.2018.00034}, ISSN={1530-2075}, month={May},}
