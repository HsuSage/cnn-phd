@INPROCEEDINGS{8373076,  author={A. A. Bahou and G. Karunaratne and R. Andri and L. Cavigelli and L. Benini}, booktitle={2018 IEEE Symposium in Low-Power and High-Speed Chips (COOL CHIPS)},  title={XNORBIN: A 95 TOp/s/W hardware accelerator for binary convolutional neural networks},  year={2018}, volume={}, number={}, pages={1-3}, abstract={Deploying state-of-the-art CNNs requires power-hungry processors and off-chip memory. This precludes the implementation of CNNs in low-power embedded systems. Recent research shows CNNs sustain extreme quantization, binarizing their weights and intermediate feature maps, thereby saving 8-32x memory and collapsing energy-intensive sum-of-products into XNOR-and-popcount operations. We present XNORBIN, a flexible accelerator for binary CNNs with computation tightly coupled to memory for aggressive data reuse supporting even non-trivial network topologies with large feature map volumes. Implemented in UMC 65nm technology XNORBIN achieves an energy efficiency of 95 TOp/s/W and an area efficiency of 2.0TOp/s/MGE at 0.8 V.}, keywords={embedded systems;low-power electronics;neural nets;power aware computing;binary convolutional neural networks;off-chip memory;low-power embedded systems;extreme quantization;flexible accelerator;aggressive data;nontrivial network topologies;feature map volumes;energy efficiency;hardware accelerator;binary CNN;weight binarization;collapsing energy-intensive sum-of-products;XNOR-and-popcount operations;Hardware;Convolutional neural networks;System-on-chip;Computational modeling;Computer architecture;Program processors}, doi={10.1109/CoolChips.2018.8373076}, ISSN={2473-4683}, month={April},}
