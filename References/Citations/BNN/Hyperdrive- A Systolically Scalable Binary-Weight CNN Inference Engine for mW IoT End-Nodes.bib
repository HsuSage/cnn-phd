@INPROCEEDINGS{8429420,  author={R. Andri and L. Cavigelli and D. Rossi and L. Benini}, booktitle={2018 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)},  title={Hyperdrive: A Systolically Scalable Binary-Weight CNN Inference Engine for mW IoT End-Nodes},  year={2018}, volume={}, number={}, pages={509-515}, abstract={Deep neural networks have achieved impressive results in computer vision and machine learning. Unfortunately, state-of-the-art networks are extremely compute-and memory-intensive which makes them unsuitable for mW-devices such as IoT end-nodes. Aggressive quantization of these networks dramatically reduces the computation and memory footprint. Binary-weight neural networks (BWNs) follow this trend, pushing weight quantization to the limit. Hardware accelerators for BWNs presented up to now have focused on core efficiency, disregarding I/O bandwidth and system-level efficiency that are crucial for deployment of accelerators in ultra-low power devices. We present Hyperdrive: a BWN accelerator dramatically reducing the I/O bandwidth exploiting a novel binary-weight streaming approach, and capable of handling high-resolution images by virtue of its systolic-scalable architecture. We achieve a 5.9 TOp/s/W system-level efficiency (i.e. including I/Os)-2.2x higher than state-of-the-art BNN accelerators, even if our core uses resource-intensive FP16 arithmetic for increased robustness.}, keywords={computer vision;feedforward neural nets;Internet of Things;learning (artificial intelligence);low-power electronics;microprocessor chips;hardware accelerators;core efficiency;I/O bandwidth;ultra-low power devices;BWN accelerator;systolic-scalable architecture;state-of-the-art BNN accelerators;resource-intensive FP16 arithmetic;TOp/s/W system-level efficiency;binary-weight streaming approach;BWN;hyperdrive;weight quantization;binary-weight neural networks;memory footprint;aggressive quantization;mW-devices;memory-intensive;machine learning;computer vision;impressive results;deep neural networks;mW IoT end-nodes;systolically scalable binary-weight CNN inference engine;Frequency modulation;Computer architecture;Quantization (signal);Neural networks;System-on-chip;Hardware;Bandwidth;Hardware Accelerator;Binary Weights Neural Networks;IoT}, doi={10.1109/ISVLSI.2018.00099}, ISSN={2159-3477}, month={July},}
