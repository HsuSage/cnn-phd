@INPROCEEDINGS{8297384,  author={X. Sun and X. Peng and P. Chen and R. Liu and J. Seo and S. Yu}, booktitle={2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC)},  title={Fully parallel RRAM synaptic array for implementing binary neural network with (+1, âˆ’1) weights and (+1, 0) neurons},  year={2018}, volume={}, number={}, pages={574-579}, abstract={Binary Neural Networks (BNNs) have been recently proposed to improve the area-/energy-efficiency of the machine/deep learning hardware accelerators, which opens an opportunity to use the technologically more mature binary RRAM devices to effectively implement the binary synaptic weights. In addition, the binary neuron activation enables using the sense amplifier instead of the analog-to-digital converter to allow bitwise communication between layers of the neural networks. However, the sense amplifier has intrinsic offset that affects the threshold of binary neuron, thus it may degrade the classification accuracy. In this work, we analyze a fully parallel RRAM synaptic array architecture that implements the fully connected layers in a convolutional neural network with (+1, -1) weights and (+1, 0) neurons. The simulation results with TSMC 65 nm PDK show that the offset of current mode sense amplifier introduces a slight accuracy loss from ~98.5% to ~97.6% for MNIST dataset. Nevertheless, the proposed fully parallel BNN architecture (P-BNN) can achieve 137.35 TOPS/W energy efficiency for the inference, improved by ~20X compared to the sequential BNN architecture (S-BNN) with row-by-row read-out scheme. Moreover, the proposed P-BNN architecture can save the chip area by ~16% as it eliminates the area overhead of MAC peripheral units in the S-BNN architecture.}, keywords={amplifiers;convolution;integrated circuit modelling;learning (artificial intelligence);neural nets;resistive RAM;machine/deep learning hardware accelerators;binary synaptic weights;binary neuron activation;fully parallel RRAM synaptic array architecture;convolutional neural network;current mode sense amplifier;fully parallel BNN architecture;binary neural network;energy efficiency;binary RRAM devices;TSMC PDK;P-BNN;sequential BNN architecture;S-BNN;row-by-row read-out scheme;MAC peripheral units;MNIST dataset;size 65.0 nm;Neurons;Computer architecture;Decoding;Training;Biological neural networks;Switches;Feature extraction}, doi={10.1109/ASPDAC.2018.8297384}, ISSN={2153-697X}, month={Jan},}
