@INPROCEEDINGS{8457667,  author={S. Kim and R. Rutenbar}, booktitle={2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},  title={Accelerator Design with Effective Resource Utilization for Binary Convolutional Neural Networks on an FPGA},  year={2018}, volume={}, number={}, pages={218-218}, abstract={In binary convolutional neural networks (BCNN), arithmetic operations are replaced by bitwise operations and the required memory size is greatly reduced, which is a good opportunity to accelerate training or inference on FPGAs. This paper proposes a BCNN architecture with a single engine that achieves high resource utilization. The proposed design deploys a large number of processing elements in parallel to increase throughput, and a forwarding scheme to increase resource utilization on the existing engine. In addition, we demonstrate a novel reuse scheme to make fully-connected layers exploit the same engine. The proposed design is combined with an inference environment for comparison and implemented on a Xilinx XCVU190 FPGA. The implemented design uses 61k look-up tables (LUTs), 45k flip-flops (FFs), and 13.9Mbit block RAM (BRAM). In addition, it achieves 61.6 GOPS/kLUT at 240MHz, which is 1.16 times higher than that of the best prior BCNN design, even though it uses a single engine without optimal configurations on each layer.}, keywords={field programmable gate arrays;flip-flops;logic design;neural chips;neural net architecture;random-access storage;table lookup;BRAM;block RAM;flip-flops;look-up tables;inference environment;fully-connected layers;processing elements;BCNN design;memory size;bitwise operations;arithmetic operations;binary convolutional neural networks;effective resource utilization;accelerator design;Xilinx XCVU190 FPGA;forwarding scheme;high resource utilization;BCNN architecture;frequency 240.0 MHz;storage capacity 13.9 Mbit;Field programmable gate arrays;Resource management;Convolutional neural networks;Engines;Computer science;Acceleration;Machine learning;Binary convolutional neural networks;High resource utilization;FPGA}, doi={10.1109/FCCM.2018.00052}, ISSN={2576-2621}, month={April},}
