@INPROCEEDINGS{7966159,  author={X. Chen and X. Hu and H. Zhou and N. Xu}, booktitle={2017 International Joint Conference on Neural Networks (IJCNN)},  title={FxpNet: Training a deep convolutional neural network in fixed-point representation},  year={2017}, volume={}, number={}, pages={2494-2501}, abstract={We introduce FxpNet, a framework to train deep convolutional neural networks with low bit-width arithmetics in both forward pass and backward pass. During training FxpNet further reduces the bit-width of stored parameters (also known as primal parameters) by adaptively updating their fixed-point formats. These primal parameters are usually represented in the full resolution of floating-point values in previous binarized and quantized neural networks. In FxpNet, during forward pass fixed-point primal weights and activations are first binarized before computation, while in backward pass all gradients are represented as low resolution fixed-point values and then accumulated to corresponding fixed-point primal parameters. To have highly efficient implementations in FPGAs, ASICs and other dedicated devices, FxpNet introduces Integer Batch Normalization (IBN) and Fixed-point ADAM (FxpADAM) methods to further reduce the required floating-point operations, which will save considerable power and chip area. The evaluation on CIFAR-10 dataset indicates the effectiveness that FxpNet with 12-bit primal parameters and 12-bit gradients achieves comparable prediction accuracy with state-of-the-art binarized and quantized neural networks.}, keywords={application specific integrated circuits;convolution;field programmable gate arrays;fixed point arithmetic;floating point arithmetic;neural nets;FxpNet;deep convolutional neural network;fixed-point representation;bit-width arithmetics;forward pass;backward pass;floating-point values;binarized neural networks;quantized neural networks;fixed-point primal weights;low resolution fixed-point values;fixed-point primal parameters;FPGAs;ASICs;integer batch normalization;IBN;fixed-point ADAM;FxpADAM;CIFAR-10 dataset;12-bit primal parameters;12-bit gradients;Quantization (signal);Training;Field programmable gate arrays;Neural networks;Convolution;Kernel;Acceleration}, doi={10.1109/IJCNN.2017.7966159}, ISSN={2161-4407}, month={May},}
