@INPROCEEDINGS{8297303,  author={Z. Zhang and D. Zhou and S. Wang and S. Kimura}, booktitle={2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC)},  title={Quad-multiplier packing based on customized floating point for convolutional neural networks on FPGA},  year={2018}, volume={}, number={}, pages={184-189}, abstract={Deep convolutional neural networks (CNNs) are widely used in many computer vision tasks. Since CNNs involve billions of computations, it is critical to reduce the resource /power consumption and improve parallelism. Compared with extensive researches on fixed point conversion for cost reduction, floating point customization has not been paid enough attention due to its higher cost than fixed point. This paper explores the customized floating point for both the training and inference of CNNs. 9-bit customized floating point is found sufficient for the training of ResNet-20 on CIFAR-10 dataset with less than 1% accuracy loss, which can also be applied to the inference of CNNs. With reduced bit-width, a computational unit (CU) based on Quad-Multiplier Packing is proposed to improve the resource efficiency of CNNs on FPGA. This design can save 87.5% DSP slices and 62.5% LUTs on Xilinx Kintex-7 platform compared to CU using 32-bit floating point. More CUs can be arranged on FPGA and higher throughput can be expected accordingly.}, keywords={computer vision;field programmable gate arrays;neural nets;fixed point conversion;point customization;reduced bit-width;computational unit;FPGA;32-bit floating point;Quad-multiplier packing;computer vision tasks;resource /power consumption;deep convolutional neural networks;CNNs;9-bit customized floating point;ResNet-20;Xilinx Kintex-7 platform;Training;Field programmable gate arrays;Kernel;Hardware;Convolutional neural networks;Computer vision}, doi={10.1109/ASPDAC.2018.8297303}, ISSN={2153-697X}, month={Jan},}
