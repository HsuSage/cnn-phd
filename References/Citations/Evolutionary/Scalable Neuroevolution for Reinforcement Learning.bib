@InProceedings{10.1007/978-3-642-33860-1_3,
author="Gomez, Faustino",
editor="Dediu, Adrian-Horia
and Mart{\'i}n-Vide, Carlos
and Truthe, Bianca",
title="Scalable Neuroevolution for Reinforcement Learning",
booktitle="Theory and Practice of Natural Computing",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="27--29",
abstract="The idea of using evolutionary computation to train artificial neural networks, or neuroevolution (NE), has now been around for over 20 years. The main appeal of this approach is that, because it does not rely on gradient information (e.g. backpropagation), it can potentially harness the universal function approximation capability of neural networks to solve reinforcement learning (RL) tasks, where there is no ``teacher'' (i.e. no targets or examples of correct behavior). Instead of incrementally adjusting the synaptic weights of a single network, the space of network parameters is searched directly according to principles inspired by natural selection: (1) encode a population of networks as strings, or genomes, (2) transform them into networks, (3) evaluate them on the task, (4) generate new, hopefully better, nets by recombining those that are most ``fit'', (5) goto step 2 until a solution is found. By evolving neural networks, NE can cope naturally with tasks that have continuous inputs and outputs, and, by evolving networks with feedback connections (recurrent networks), it can tackle more general tasks that require memory.",
isbn="978-3-642-33860-1"
}

