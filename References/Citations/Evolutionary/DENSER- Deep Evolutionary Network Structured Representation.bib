@Article{Assuncao2018,
author="Assun{\c{c}}{\~a}o, Filipe
and Louren{\c{c}}o, Nuno
and Machado, Penousal
and Ribeiro, Bernardete",
title="DENSER: deep evolutionary network structured representation",
journal="Genetic Programming and Evolvable Machines",
year="2018",
month="Sep",
day="27",
abstract="Deep evolutionary network structured representation (DENSER) is a novel evolutionary approach for the automatic generation of deep neural networks (DNNs) which combines the principles of genetic algorithms (GAs) with those of dynamic structured grammatical evolution (DSGE). The GA-level encodes the macro structure of evolution, i.e., the layers, learning, and/or data augmentation methods (among others); the DSGE-level specifies the parameters of each GA evolutionary unit and the valid range of the parameters. The use of a grammar makes DENSER a general purpose framework for generating DNNs: one just needs to adapt the grammar to be able to deal with different network and layer types, problems, or even to change the range of the parameters. DENSER is tested on the automatic generation of convolutional neural networks (CNNs) for the CIFAR-10 dataset, with the best performing networks reaching accuracies of up to 95.22{\%}. Furthermore, we take the fittest networks evolved on the CIFAR-10, and apply them to classify MNIST, Fashion-MNIST, SVHN, Rectangles, and CIFAR-100. The results show that the DNNs discovered by DENSER during evolution generalise, are robust, and scale. The most impressive result is the 78.75{\%} classification accuracy on the CIFAR-100 dataset, which, to the best of our knowledge, sets a new state-of-the-art on methods that seek to automatically design CNNs.",
issn="1573-7632",
doi="10.1007/s10710-018-9339-y",
url="https://doi.org/10.1007/s10710-018-9339-y"
}

