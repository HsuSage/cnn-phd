@Inbook{Moussa2006,
author="Moussa, Medhat
and Areibi, Shawki
and Nichols, Kristian",
editor="Omondi, Amos R.
and Rajapakse, Jagath C.",
title="On the Arithmetic Precision for Implementing Back-Propagation Networks on FPGA: A Case Study",
bookTitle="FPGA Implementations of Neural Networks",
year="2006",
publisher="Springer US",
address="Boston, MA",
pages="37--61",
abstract="Artificial Neural Networks (ANNs) are inherently parallel architectures which represent a natural fit for custom implementation on FPGAs. One important implementation issue is to determine the numerical precision format that allows an optimum tradeoff between precision and implementation areas. Standard single or double precision floating-point representations minimize quantization errors while requiring significant hardware resources. Less precise fixed-point representation may require less hardware resources but add quantization errors that may prevent learning from taking place, especially in regression problems. This chapter examines this issue and reports on a recent experiment where we implemented a Multi-layer perceptron (MLP) on an FPGA using both fixed and floating point precision. Results show that the fixed-point MLP implementation was over 12x greater in speed, over 13x smaller in area, and achieves far greater processing density compared to the floating-point FPGA-based MLP.",
isbn="978-0-387-28487-3",
doi="10.1007/0-387-28487-7_2",
url="https://doi.org/10.1007/0-387-28487-7_2"
}

