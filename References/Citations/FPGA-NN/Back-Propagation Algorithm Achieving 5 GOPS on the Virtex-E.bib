@Inbook{Paul2006,
author="Paul, Kolin
and Rajopadhye, Sanjay",
editor="Omondi, Amos R.
and Rajapakse, Jagath C.",
title="Back-Propagation Algorithm Achieving 5 Gops on the Virtex-E",
bookTitle="FPGA Implementations of Neural Networks",
year="2006",
publisher="Springer US",
address="Boston, MA",
pages="137--165",
abstract="Back propagation is a well known technique used in the implementation of artificial neural networks. The algorithm can be described essentially as a sequence of matrix vector multiplications and outer product operations interspersed with the application of a point wise non linear function. The algorithm is compute intensive and lends itself to a high degree of parallelism. These features motivate a systolic design of hardware to implement the Back Propagation algorithm. We present in this chapter a new systolic architecture for the complete back propagation algorithm. For a neural network with N input neurons, P hidden layer neurons and M output neurons, the proposed architecture with P processors, has a running time of (2N + 2M + P + max(M,P)) for each training set vector. This is the first such implementation of the back propagation algorithm which completely parallelizes the entire computation of learning phase. The array has been implemented on an Annapolis FPGA based coprocessor and it achieves very favorable performance with range of 5 GOPS. The proposed new design targets Virtex boards.",
isbn="978-0-387-28487-3",
doi="10.1007/0-387-28487-7_5",
url="https://doi.org/10.1007/0-387-28487-7_5"
}

