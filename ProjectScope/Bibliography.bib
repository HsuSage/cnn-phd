%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for RHVT at 2018-05-19 23:32:05 +1200 


%% Saved with string encoding Unicode (UTF-8) 



@article{2015arXiv150303562C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150303562C},
	Archiveprefix = {arXiv},
	Author = {{Cheng}, Z. and {Soudry}, D. and {Mao}, Z. and {Lan}, Z.},
	Date-Added = {2018-05-19 11:32:04 +0000},
	Date-Modified = {2018-05-19 11:32:04 +0000},
	Eprint = {1503.03562},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	Month = mar,
	Title = {{Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation}},
	Year = 2015}

@article{2016arXiv161103530Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161103530Z},
	Archiveprefix = {arXiv},
	Author = {{Zhang}, C. and {Bengio}, S. and {Hardt}, M. and {Recht}, B. and {Vinyals}, O.},
	Date-Added = {2018-05-19 05:48:30 +0000},
	Date-Modified = {2018-05-19 05:48:30 +0000},
	Eprint = {1611.03530},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning},
	Month = nov,
	Primaryclass = {cs.LG},
	Title = {{Understanding deep learning requires rethinking generalization}},
	Year = 2016}

@article{2017arXiv170107875A,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170107875A},
	Archiveprefix = {arXiv},
	Author = {{Arjovsky}, M. and {Chintala}, S. and {Bottou}, L.},
	Date-Added = {2018-05-19 04:48:30 +0000},
	Date-Modified = {2018-05-19 04:48:30 +0000},
	Eprint = {1701.07875},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Learning},
	Month = jan,
	Primaryclass = {stat.ML},
	Title = {{Wasserstein GAN}},
	Year = 2017}

@article{2015arXiv151202479M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151202479M},
	Archiveprefix = {arXiv},
	Author = {{Montavon}, G. and {Bach}, S. and {Binder}, A. and {Samek}, W. and {M{\"u}ller}, K.-R.},
	Date-Added = {2018-05-16 10:38:31 +0000},
	Date-Modified = {2018-05-16 10:38:31 +0000},
	Eprint = {1512.02479},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = dec,
	Primaryclass = {cs.LG},
	Title = {{Explaining NonLinear Classification Decisions with Deep Taylor Decomposition}},
	Year = 2015}

@article{journal.pone.0130140,
	Abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
	Author = {Bach, Sebastian AND Binder, Alexander AND Montavon, Gr{\'e}goire AND Klauschen, Frederick AND M{\"u}ller, Klaus-Robert AND Samek, Wojciech},
	Date-Added = {2018-05-16 09:58:57 +0000},
	Date-Modified = {2018-05-16 09:59:52 +0000},
	Doi = {10.1371/journal.pone.0130140},
	Journal = {PLOS ONE},
	Keywords = {DNN, Taylor Series, MNIST, ImageNet, Pixel-wise},
	Month = {07},
	Number = {7},
	Pages = {1-46},
	Publisher = {Public Library of Science},
	Title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
	Url = {https://doi.org/10.1371/journal.pone.0130140},
	Volume = {10},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1371/journal.pone.0130140}}

@article{kim51bitwise,
	Author = {Kim, Minje and Smaragdis, Paris},
	Date-Added = {2018-05-16 00:48:16 +0000},
	Date-Modified = {2018-05-16 00:49:10 +0000},
	Journal = {Urbana},
	Keywords = {BNN, QaD, IBM, Signals, Denoise},
	Pages = {61801},
	Title = {Bitwise Neural Networks for Efficient Single-Channel Source Separation},
	Volume = {51}}

@article{2018arXiv180409154G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180409154G},
	Archiveprefix = {arXiv},
	Author = {{Giacomello}, E. and {Lanzi}, P.~L. and {Loiacono}, D.},
	Date-Added = {2018-05-15 11:53:51 +0000},
	Date-Modified = {2018-05-15 11:53:51 +0000},
	Eprint = {1804.09154},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Human-Computer Interaction, Statistics - Machine Learning},
	Month = apr,
	Primaryclass = {cs.LG},
	Title = {{DOOM Level Generation using Generative Adversarial Networks}},
	Year = 2018,
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QgS4uL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OL09uIFBpeGVsLVdpc2UgRXhwbGFuYXRpb25zIGZvciBOb24tTGluZWFyIENsYXNzaWZpZXIgRGVjaXNpb25zIGJ5IExheWVyLVdpc2UgUmVsZXZhbmNlIFByb3BhZ2F0aW9uLmJpYtIXCxgZV05TLmRhdGFPEQLMAAAAAALMAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fT24gUGl4ZWwtV2lzZSBFeHBsI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAAAk5OAAIAly86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpOTjpPbiBQaXhlbC1XaXNlIEV4cGxhbmF0aW9ucyBmb3IgTm9uLUxpbmVhciBDbGFzc2lmaWVyIERlY2lzaW9ucyBieSBMYXllci1XaXNlIFJlbGV2YW5jZSBQcm9wYWdhdGlvbi5iaWIAAA4AzgBmAE8AbgAgAFAAaQB4AGUAbAAtAFcAaQBzAGUAIABFAHgAcABsAGEAbgBhAHQAaQBvAG4AcwAgAGYAbwByACAATgBvAG4ALQBMAGkAbgBlAGEAcgAgAEMAbABhAHMAcwBpAGYAaQBlAHIAIABEAGUAYwBpAHMAaQBvAG4AcwAgAGIAeQAgAEwAYQB5AGUAcgAtAFcAaQBzAGUAIABSAGUAbABlAHYAYQBuAGMAZQAgAFAAcgBvAHAAYQBnAGEAdABpAG8AbgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAlVVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vT24gUGl4ZWwtV2lzZSBFeHBsYW5hdGlvbnMgZm9yIE5vbi1MaW5lYXIgQ2xhc3NpZmllciBEZWNpc2lvbnMgYnkgTGF5ZXItV2lzZSBSZWxldmFuY2UgUHJvcGFnYXRpb24uYmliAAATAAEvAAAVAAIAC///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOARIBFwEfA+8D8QP2BAEECgQYBBwEIwQsBDEEPgRBBFMEVgRbAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAABF0=}}

@inproceedings{5234726,
	Author = {X. Chen and Q. Ma and T. Alkharobi},
	Booktitle = {2009 2nd IEEE International Conference on Computer Science and Information Technology},
	Date-Added = {2018-05-15 11:27:59 +0000},
	Date-Modified = {2018-05-15 11:27:59 +0000},
	Doi = {10.1109/ICCSIT.2009.5234726},
	Keywords = {Fourier series;data mining;pattern clustering;radial basis function networks;Fourier component neural network;Gauss series clustering neural network;Taylor component neural network;Taylor series;mining;radial basis function neuron;Computer science;Educational institutions;Electronic mail;Gaussian processes;Information science;Input variables;Neural networks;Neurons;Taylor series;Transfer functions;Fourier component neural network;Gauss series Clustering neural network;Taylor component neural network;Taylor series neural network;prediction;stock price},
	Month = {Aug},
	Pages = {291-294},
	Title = {New neural networks based on Taylor series and their research},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICCSIT.2009.5234726}}

@inproceedings{li2013,
	Abstract = {This study adopts popular back-propagation neural network to make one-period-ahead prediction of the stock price. A model based on Taylor series by using both fundamental and technical indicators EPS and MACD as input data is built for an empirical study. Leading Taiwanese companies in non-hi-tech industry such as Formosa Plastics, Yieh Phui Steel, Evergreen Marine, and Chang Hwa Bank are picked as targets to analyze their reasonable prices and moving trends. The performance of this model shows remarkable return and high accuracy in making long/short strategies.},
	Author = {Li, Jung Bin and Wu, Chien Ho},
	Booktitle = {Innovation for Applied Science and Technology},
	Date-Added = {2018-05-15 11:27:42 +0000},
	Date-Modified = {2018-05-15 11:27:42 +0000},
	Doi = {10.4028/www.scientific.net/AMM.284-287.3020},
	Keywords = {Neural Network (NN), BPN, Taylor Series, Price Forecast},
	Month = {3},
	Pages = {3020--3024},
	Publisher = {Trans Tech Publications},
	Series = {Applied Mechanics and Materials},
	Title = {An Efficient Neural Network Model with Taylor Series-Based Data Pre-Processing for Stock Price Forecast},
	Volume = {284},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.4028/www.scientific.net/AMM.284-287.3020}}

@article{2016arXiv160205897D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160205897D},
	Archiveprefix = {arXiv},
	Author = {{Daniely}, A. and {Frostig}, R. and {Singer}, Y.},
	Date-Added = {2018-05-15 11:27:25 +0000},
	Date-Modified = {2018-05-15 11:27:25 +0000},
	Eprint = {1602.05897},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms, Statistics - Machine Learning},
	Month = feb,
	Primaryclass = {cs.LG},
	Title = {{Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity}},
	Year = 2016}

@article{doi:10.1002/mma.2641,
	Abstract = {This paper focuses on learning algorithms for approximating functional data that are chosen from some Hilbert spaces. An effective algorithm, called Hilbert parallel overrelaxation backpropagation (HPORBP) algorithm, is proposed for training the Hilbert feedforward neural networks that are extensions of feedforward neural networks from Euclidean space to some Hilbert spaces. Furthermore, the convergence of the iterative HPORBP algorithm is analyzed, and a deterministic convergence theorem is proposed for the HPORBP algorithm on the basis of the perturbation results of Mangasarian and Solodov. Some experimental results of learning functional data on some Hilbert spaces illustrate the convergence theorem and show that the proposed HPORBP algorithm has a better accuracy than the Hilbert backpropagation algorithm. Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
	Author = {Zhao Jianwei},
	Date-Added = {2018-05-15 11:26:53 +0000},
	Date-Modified = {2018-05-15 11:26:53 +0000},
	Doi = {10.1002/mma.2641},
	Eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/mma.2641},
	Journal = {Mathematical Methods in the Applied Sciences},
	Keywords = {functional data, feedforward neural network, learning algorithm, convergence},
	Number = {17},
	Pages = {2111-2121},
	Title = {Functional data learning by Hilbert feedforward neural networks},
	Url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mma.2641},
	Volume = {35},
	Bdsk-Url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mma.2641},
	Bdsk-Url-2 = {https://doi.org/10.1002/mma.2641}}

@article{CASTRO2005967,
	Abstract = {The Fuzzy ARTMAP algorithm has been proven to be one of the premier neural network architectures for classification problems. One of the properties of Fuzzy ARTMAP, which can be both an asset and a liability, is its capacity to produce new nodes (templates) on demand to represent classification categories. This property allows Fuzzy ARTMAP to automatically adapt to the database without having to a priori specify its network size. On the other hand, it has the undesirable side effect that large databases might produce a large network size (node proliferation) that can dramatically slow down the training speed of the algorithm. To address the slow convergence speed of Fuzzy ARTMAP for large database problems, we propose the use of space-filling curves, specifically the Hilbert space-filling curves (HSFC). Hilbert space-filling curves allow us to divide the problem into smaller sub-problems, each focusing on a smaller than the original dataset. For learning each partition of data, a different Fuzzy ARTMAP network is used. Through this divide-and-conquer approach we are avoiding the node proliferation problem, and consequently we speedup Fuzzy ARTMAP's training. Results have been produced for a two-class, 16-dimensional Gaussian data, and on the Forest database, available at the UCI repository. Our results indicate that the Hilbert space-filling curve approach reduces the time that it takes to train Fuzzy ARTMAP without affecting the generalization performance attained by Fuzzy ARTMAP trained on the original large dataset. Given that the resulting smaller datasets that the HSFC approach produces can independently be learned by different Fuzzy ARTMAP networks, we have also implemented and tested a parallel implementation of this approach on a Beowulf cluster of workstations that further speeds up Fuzzy ARTMAP's convergence to a solution for large database problems.},
	Author = {Jos{\'e} Castro and Michael Georgiopoulos and Ronald Demara and Avelino Gonzalez},
	Date-Added = {2018-05-15 11:26:41 +0000},
	Date-Modified = {2018-05-15 11:26:41 +0000},
	Doi = {https://doi.org/10.1016/j.neunet.2005.01.007},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {Fuzzy-ARTMAP, Hilbert space-filling curve, Data mining, Data-partitioning},
	Number = {7},
	Pages = {967 - 984},
	Title = {Data-partitioning using the Hilbert space filling curves: Effect on the speed of convergence of Fuzzy ARTMAP for large database problems},
	Url = {http://www.sciencedirect.com/science/article/pii/S089360800500047X},
	Volume = {18},
	Year = {2005},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S089360800500047X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neunet.2005.01.007}}

@article{2018arXiv180501934C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180501934C},
	Archiveprefix = {arXiv},
	Author = {{Chen}, C. and {Chen}, Q. and {Xu}, J. and {Koltun}, V.},
	Date-Added = {2018-05-15 11:26:25 +0000},
	Date-Modified = {2018-05-15 11:26:25 +0000},
	Eprint = {1805.01934},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Learning},
	Month = may,
	Primaryclass = {cs.CV},
	Title = {{Learning to See in the Dark}},
	Year = 2018}

@article{2017arXiv171111294L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171111294L},
	Archiveprefix = {arXiv},
	Author = {{Lin}, X. and {Zhao}, C. and {Pan}, W.},
	Date-Added = {2018-05-15 11:26:04 +0000},
	Date-Modified = {2018-05-15 11:26:04 +0000},
	Eprint = {1711.11294},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = nov,
	Primaryclass = {cs.LG},
	Title = {{Towards Accurate Binary Convolutional Neural Network}},
	Year = 2017}

@article{2017arXiv170906206Y,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170906206Y},
	Archiveprefix = {arXiv},
	Author = {{Yin}, S. and {Venkataramanaiah}, S.~K. and {Chen}, G.~K. and {Krishnamurthy}, R. and {Cao}, Y. and {Chakrabarti}, C. and {Seo}, J.-s.},
	Date-Added = {2018-05-15 11:24:06 +0000},
	Date-Modified = {2018-05-15 11:24:06 +0000},
	Eprint = {1709.06206},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing},
	Month = sep,
	Title = {{Algorithm and Hardware Design of Discrete-Time Spiking Neural Networks Based on Back Propagation with Binary Activations}},
	Year = 2017}

@inproceedings{5949465,
	Author = {J. Ranhel and C. V. Lima and J. L. R. Monteiro and J. E. Kogler and M. L. Netto},
	Booktitle = {2011 IEEE Symposium on Foundations of Computational Intelligence (FOCI)},
	Date-Added = {2018-05-15 11:24:06 +0000},
	Date-Modified = {2018-05-15 11:24:06 +0000},
	Doi = {10.1109/FOCI.2011.5949465},
	Keywords = {neural nets;storage management;PNG attributes;binary counters;bistable memory;parallel computing;polychronous group;spiking neural network;Biological system modeling;Computational modeling;Delay;Fires;Firing;Kernel;Neurons;bistable neural memory;neural counters;neural hierarchical organization;neural stack counter;polychronization;spiking neural networks},
	Month = {April},
	Pages = {66-73},
	Title = {Bistable memory and binary counters in spiking neural network},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/FOCI.2011.5949465}}

@article{2016arXiv161105128Y,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161105128Y},
	Archiveprefix = {arXiv},
	Author = {{Yang}, T.-J. and {Chen}, Y.-H. and {Sze}, V.},
	Date-Added = {2018-05-11 02:51:58 +0000},
	Date-Modified = {2018-05-11 02:51:58 +0000},
	Eprint = {1611.05128},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = nov,
	Primaryclass = {cs.CV},
	Title = {{Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning}},
	Year = 2016}

@inproceedings{Chen2018UnderstandingTL,
	Author = {Yu-hsin Chen and Tien-Ju Yang and Joel S. Emer and Vivienne Sze},
	Date-Added = {2018-05-11 00:57:52 +0000},
	Date-Modified = {2018-05-11 00:58:11 +0000},
	Keywords = {NN, DNN, Hardware, FPGA, ASIC},
	Title = {Understanding the Limitations of Existing Energy-Efficient Design Approaches for Deep Neural Networks},
	Year = {2018}}

@article{2017arXiv170309039S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170309039S},
	Archiveprefix = {arXiv},
	Author = {{Sze}, V. and {Chen}, Y.-H. and {Yang}, T.-J. and {Emer}, J.},
	Date-Added = {2018-05-10 05:28:12 +0000},
	Date-Modified = {2018-05-10 05:28:12 +0000},
	Eprint = {1703.09039},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = mar,
	Primaryclass = {cs.CV},
	Title = {{Efficient Processing of Deep Neural Networks: A Tutorial and Survey}},
	Year = 2017}

@article{2016arXiv161207625S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161207625S},
	Archiveprefix = {arXiv},
	Author = {{Sze}, V. and {Chen}, Y.-H. and {Emer}, J. and {Suleiman}, A. and {Zhang}, Z.},
	Date-Added = {2018-05-10 05:20:22 +0000},
	Date-Modified = {2018-05-10 05:20:22 +0000},
	Eprint = {1612.07625},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = dec,
	Primaryclass = {cs.CV},
	Title = {{Hardware for Machine Learning: Challenges and Opportunities}},
	Year = 2016}

@article{2018arXiv180403230Y,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180403230Y},
	Archiveprefix = {arXiv},
	Author = {{Yang}, T.-J. and {Howard}, A. and {Chen}, B. and {Zhang}, X. and {Go}, A. and {Sze}, V. and {Adam}, H.},
	Date-Added = {2018-05-10 04:40:55 +0000},
	Date-Modified = {2018-05-10 04:40:55 +0000},
	Eprint = {1804.03230},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = apr,
	Primaryclass = {cs.CV},
	Title = {{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications}},
	Year = 2018}

@article{7738524,
	Author = {Y. H. Chen and T. Krishna and J. S. Emer and V. Sze},
	Date-Added = {2018-05-10 03:54:03 +0000},
	Date-Modified = {2018-05-10 03:54:03 +0000},
	Doi = {10.1109/JSSC.2016.2616357},
	Issn = {0018-9200},
	Journal = {IEEE Journal of Solid-State Circuits},
	Keywords = {DRAM chips;data flow computing;energy conservation;feedforward neural nets;learning (artificial intelligence);neural net architecture;power aware computing;reconfigurable architectures;AI systems;AlexNet;CNN shapes;DRAM accesses;Eyeriss;MAC;RS dataflow reconfiguration;accelerator chip;convolutional layers;data movement energy cost;dataflow processing;deep convolutional neural networks;energy efficiency;energy-efficient reconfigurable accelerator;multiply and accumulation;off-chip DRAM;reconfiguring architecture;row stationary;spatial architecture;Clocks;Computer architecture;Hardware;Neural networks;Random access memory;Shape;Throughput;Convolutional neural networks (CNNs);dataflow processing;deep learning;energy-efficient accelerators;spatial architecture},
	Month = {Jan},
	Number = {1},
	Pages = {127-138},
	Title = {Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks},
	Volume = {52},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/JSSC.2016.2616357}}

@article{2014arXiv1412.6980K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1412.6980K},
	Archiveprefix = {arXiv},
	Author = {{Kingma}, D.~P. and {Ba}, J.},
	Date-Added = {2018-05-09 11:47:33 +0000},
	Date-Modified = {2018-05-09 11:47:33 +0000},
	Eprint = {1412.6980},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning},
	Month = dec,
	Primaryclass = {cs.LG},
	Title = {{Adam: A Method for Stochastic Optimization}},
	Year = 2014}

@inproceedings{8302078,
	Author = {X. Xu and J. Amaro and S. Caulfield and A. Forembski and G. Falcao and D. Moloney},
	Booktitle = {2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
	Date-Added = {2018-05-09 11:20:24 +0000},
	Date-Modified = {2018-05-09 11:20:24 +0000},
	Doi = {10.1109/CISP-BMEI.2017.8302078},
	Keywords = {computer vision;convolution;feedforward neural nets;image classification;image processing;multiprocessing systems;object recognition;stereo image processing;2D Convolutional Neural Networks;3D volumetric representation;CNNs;Movidius Neural Compute Stick;USB;VOLA;Volumetric Accelerator;computational requirements;computer vision;dedicated CNN hardware blocks;low-power processing unit;synthetic 3D voxelized point-clouds generation method;trained model;training data;volumetric data;voxelized point-clouds classification;Computational modeling;Graphics processing units;Object recognition;Solid modeling;Task analysis;Three-dimensional displays;Training;Convolutional Neural Networks;Embedded Systems;Point-clouds},
	Month = {Oct},
	Pages = {1-7},
	Title = {Convolutional neural network on neural compute stick for voxelized point-clouds classification},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/CISP-BMEI.2017.8302078}}

@article{2015arXiv151003009L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151003009L},
	Archiveprefix = {arXiv},
	Author = {{Lin}, Z. and {Courbariaux}, M. and {Memisevic}, R. and {Bengio}, Y.},
	Date-Added = {2018-05-09 11:05:15 +0000},
	Date-Modified = {2018-05-09 11:05:15 +0000},
	Eprint = {1510.03009},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = oct,
	Primaryclass = {cs.LG},
	Title = {{Neural Networks with Few Multiplications}},
	Year = 2015}

@inproceedings{7929192,
	Author = {E. Nurvitadhi and D. Sheffield and Jaewoong Sim and A. Mishra and G. Venkatesh and D. Marr},
	Booktitle = {2016 International Conference on Field-Programmable Technology (FPT)},
	Date-Added = {2018-05-06 09:18:40 +0000},
	Date-Modified = {2018-05-06 09:18:40 +0000},
	Doi = {10.1109/FPT.2016.7929192},
	Keywords = {application specific integrated circuits;field programmable gate arrays;graphics processing units;microprocessor chips;neural nets;ASIC;Aria 10 FPGA;BNN hardware accelerator design;CPU;DNN;GPU;binarized neural networks;deep neural network;hardware acceleration;Biological neural networks;Field programmable gate arrays;Graphics processing units;Hardware;Neurons;Random access memory;System-on-chip;ASIC;CPU;Deep learning;FPGA;GPU;binarized neural networks;data analytics;hardware accelerator},
	Month = {Dec},
	Pages = {77-84},
	Title = {Accelerating Binarized Neural Networks: Comparison of FPGA, CPU, GPU, and ASIC},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/FPT.2016.7929192}}

@inproceedings{Zhao:2017:ABC:3020078.3021741,
	Acmid = {3021741},
	Address = {New York, NY, USA},
	Author = {Zhao, Ritchie and Song, Weinan and Zhang, Wentao and Xing, Tianwei and Lin, Jeng-Hau and Srivastava, Mani and Gupta, Rajesh and Zhang, Zhiru},
	Booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-05-06 08:57:34 +0000},
	Date-Modified = {2018-05-06 08:57:34 +0000},
	Doi = {10.1145/3020078.3021741},
	Isbn = {978-1-4503-4354-1},
	Keywords = {FPGAs, binarized, binarized convolutional networks, deep learning, high-level synthesis, reconfigurable computing},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {15--24},
	Publisher = {ACM},
	Series = {FPGA '17},
	Title = {Accelerating Binarized Convolutional Neural Networks with Software-Programmable FPGAs},
	Url = {http://doi.acm.org/10.1145/3020078.3021741},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021741},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021741}}

@article{2016arXiv160202830C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160202830C},
	Archiveprefix = {arXiv},
	Author = {{Courbariaux}, M. and {Hubara}, I. and {Soudry}, D. and {El-Yaniv}, R. and {Bengio}, Y.},
	Date-Added = {2018-05-06 05:56:46 +0000},
	Date-Modified = {2018-05-06 05:56:46 +0000},
	Eprint = {1602.02830},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning},
	Month = feb,
	Primaryclass = {cs.LG},
	Title = {{Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1}},
	Year = 2016}

@article{2016arXiv160305279R,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160305279R},
	Archiveprefix = {arXiv},
	Author = {{Rastegari}, M. and {Ordonez}, V. and {Redmon}, J. and {Farhadi}, A.},
	Date-Added = {2018-05-06 05:43:43 +0000},
	Date-Modified = {2018-05-06 05:43:43 +0000},
	Eprint = {1603.05279},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = mar,
	Primaryclass = {cs.CV},
	Title = {{XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks}},
	Year = 2016}

@article{2016arXiv160106071K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160106071K},
	Archiveprefix = {arXiv},
	Author = {{Kim}, M. and {Smaragdis}, P.},
	Date-Added = {2018-05-06 05:27:19 +0000},
	Date-Modified = {2018-05-06 05:27:19 +0000},
	Eprint = {1601.06071},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	Month = jan,
	Primaryclass = {cs.LG},
	Title = {{Bitwise Neural Networks}},
	Year = 2016}

@inbook{Baez2011,
	Abstract = {In physics, Feynman diagrams are used to reason about quantum processes. In the 1980s, it became clear that underlying these diagrams is a powerful analogy between quantum physics and topology. Namely, a linear operator behaves very much like a ``cobordism'': a manifold representing spacetime, going between two manifolds representing space. This led to a burst of work on topological quantum field theory and ``quantum topology''. But this was just the beginning: similar diagrams can be used to reason about logic, where they represent proofs, and computation, where they represent programs. With the rise of interest in quantum cryptography and quantum computation, it became clear that there is extensive network of analogies between physics, topology, logic and computation. In this expository paper, we make some of these analogies precise using the concept of ``closed symmetric monoidal category''. We assume no prior knowledge of category theory, proof theory or computer science.},
	Address = {Berlin, Heidelberg},
	Author = {Baez, J. and Stay, M.},
	Booktitle = {New Structures for Physics},
	Date-Added = {2018-04-29 11:38:03 +0000},
	Date-Modified = {2018-04-29 11:38:23 +0000},
	Doi = {10.1007/978-3-642-12821-9_2},
	Editor = {Coecke, Bob},
	Isbn = {978-3-642-12821-9},
	Keywords = {Category Theory, Physics, Logic, Computability},
	Pages = {95--172},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Physics, Topology, Logic and Computation: A Rosetta Stone},
	Url = {https://doi.org/10.1007/978-3-642-12821-9_2},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-642-12821-9_2}}

@article{2017arXiv170404865J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170404865J},
	Archiveprefix = {arXiv},
	Author = {{Juefei-Xu}, F. and {Naresh Boddeti}, V. and {Savvides}, M.},
	Date-Added = {2018-04-28 05:48:26 +0000},
	Date-Modified = {2018-04-28 05:48:26 +0000},
	Eprint = {1704.04865},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	Month = apr,
	Primaryclass = {cs.CV},
	Title = {{Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking}},
	Year = 2017}

@article{5607329,
	Abstract = {This paper presents the development and implementation of a generalized backpropagation multilayer perceptron (MLP) architecture described in VLSI hardware description language (VHDL). The development of hardware platforms has been complicated by the high hardware cost and quantity of the arithmetic operations required in online artificial neural networks (ANNs), i.e., general purpose ANNs with learning capability. Besides, there remains a dearth of hardware platforms for design space exploration, fast prototyping, and testing of these networks. Our general purpose architecture seeks to fill that gap and at the same time serve as a tool to gain a better understanding of issues unique to ANNs implemented in hardware, particularly using field programmable gate array (FPGA). The challenge is thus to find an architecture that minimizes hardware costs, while maximizing performance, accuracy, and parameterization. This work describes a platform that offers a high degree of parameterization, while maintaining generalized network design with performance comparable to other hardware-based MLP implementations. Application of the hardware implementation of ANN with backpropagation learning algorithm for a realistic application is also presented.},
	Author = {A. Gomperts and A. Ukil and F. Zurfluh},
	Date-Added = {2018-04-28 04:52:24 +0000},
	Date-Modified = {2018-04-28 04:52:24 +0000},
	Doi = {10.1109/TII.2010.2085006},
	Issn = {1551-3203},
	Journal = {IEEE Transactions on Industrial Informatics},
	Keywords = {backpropagation;field programmable gate arrays;hardware description languages;multilayer perceptrons;FPGA;VLSI hardware description language;arithmetic operation;artificial neural network;backpropagation multilayer perceptron;fast prototyping;field programmable gate array;general purpose neural network;hardware-based MLP;learning capability;online application;space exploration;Backpropagation;NIR spectra calibration;VHDL;Xilinx FPGA;field programmable gate array (FPGA);hardware implementation;multilayer perceptron;neural network;spectroscopy},
	Month = {Feb},
	Number = {1},
	Pages = {78-89},
	Title = {Development and Implementation of Parameterized FPGA-Based General Purpose Neural Networks for Online Applications},
	Volume = {7},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/TII.2010.2085006}}

@inproceedings{Sun:2018:FPR:3201607.3201741,
	Acmid = {3201741},
	Address = {Piscataway, NJ, USA},
	Author = {Sun, Xiaoyu and Peng, Xiaochen and Chen, Pai-Yu and Liu, Rui and Seo, Jae-sun and Yu, Shimeng},
	Booktitle = {Proceedings of the 23rd Asia and South Pacific Design Automation Conference},
	Date-Added = {2018-04-28 04:34:43 +0000},
	Date-Modified = {2018-04-28 04:35:22 +0000},
	Keywords = {BNN, P-BNN, CSM, MNIST},
	Location = {Jeju, Republic of Korea},
	Numpages = {6},
	Pages = {574--579},
	Publisher = {IEEE Press},
	Series = {ASPDAC '18},
	Title = {Fully Parallel RRAM Synaptic Array for Implementing Binary Neural Network with (+1, \&Minus;1) Weights and (+1, 0) Neurons},
	Url = {http://dl.acm.org/citation.cfm?id=3201607.3201741},
	Year = {2018},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3201607.3201741}}

@article{2018arXiv180200904L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180200904L},
	Archiveprefix = {arXiv},
	Author = {{Li}, Y. and {Ren}, F.},
	Date-Added = {2018-04-28 04:09:52 +0000},
	Date-Modified = {2018-04-28 04:09:52 +0000},
	Eprint = {1802.00904},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = feb,
	Primaryclass = {cs.CV},
	Title = {{Build a Compact Binary Neural Network through Bit-level Sensitivity and Data Pruning}},
	Year = 2018}

@article{2017arXiv171110761L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171110761L},
	Archiveprefix = {arXiv},
	Author = {{Leroux}, S. and {Bohez}, S. and {Verbelen}, T. and {Vankeirsbilck}, B. and {Simoens}, P. and {Dhoedt}, B.},
	Date-Added = {2018-04-28 03:56:37 +0000},
	Date-Modified = {2018-04-28 03:56:37 +0000},
	Eprint = {1711.10761},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	Month = nov,
	Title = {{Transfer Learning with Binary Neural Networks}},
	Year = 2017}

@inproceedings{8052915,
	Abstract = {As a popular deep learning technique, convolutional neural network has been widely used in many tasks such as image classification and object recognition. Convolutional neural network exploits spatial correlations in the images by performing convolution operations in local receptive fields. Convolutional neural networks are preferred over fully connected neural networks because they have fewer weights and are easier to train. Many research works have been conducted to reduce the computational complexity and memory requirements of convolutional neural network, to make it applicable to the low-power embedded applications with limited memories. This paper presents the architecture design of convolutional neural network with binary weights and activations, also known as binary neural network, on an FPGA platform. Weights and input activations are binarized with only two values, +1 and -1. This reduces all the fixed point multiplication operations in convolutional layers and fully connected layers to 1-bit XNOR operations. The proposed design uses only on-chip memories. Furthermore, an efficient implementation of batch normalization operation is introduced. When evaluating the CIFAR-10 benchmark, the proposed FPGA design can achieve a processing rate of 332,158 images per second with with accuracy of 86.06% using 1-bit quantized weights and activations.},
	Author = {Y. Zhou and S. Redkar and X. Huang},
	Booktitle = {2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)},
	Date-Added = {2018-04-28 03:37:36 +0000},
	Date-Modified = {2018-04-28 03:37:36 +0000},
	Doi = {10.1109/MWSCAS.2017.8052915},
	Keywords = {field programmable gate arrays;fixed point arithmetic;image classification;learning (artificial intelligence);neural nets;object recognition;quantisation (signal);1-bit XNOR operation;CIFAR-10 benchmark;FPGA platform;batch normalization operation;computational complexity reduction;convolutional neural network;deep learning binary neural network;fixed point multiplication operation;local receptive fields;low-power embedded applications;memory requirement reduction;on-chip memories;spatial correlation;Biological neural networks;Convolution;Field programmable gate arrays;Hardware;Memory management;Training},
	Month = {Aug},
	Pages = {281-284},
	Title = {Deep learning binary neural network on an FPGA},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/MWSCAS.2017.8052915}}

@article{2017arXiv170905306G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170905306G},
	Archiveprefix = {arXiv},
	Author = {{Guan}, T. and {Zeng}, X. and {Seok}, M.},
	Date-Added = {2018-04-28 03:26:42 +0000},
	Date-Modified = {2018-04-28 03:26:42 +0000},
	Eprint = {1709.05306},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing},
	Month = sep,
	Title = {{Recursive Binary Neural Network Learning Model with 2.28b/Weight Storage Requirement}},
	Year = 2017}

@article{2015arXiv150201852H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150201852H},
	Archiveprefix = {arXiv},
	Author = {{He}, K. and {Zhang}, X. and {Ren}, S. and {Sun}, J.},
	Date-Added = {2018-04-28 02:37:55 +0000},
	Date-Modified = {2018-04-28 02:37:55 +0000},
	Eprint = {1502.01852},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Learning},
	Month = feb,
	Primaryclass = {cs.CV},
	Title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
	Year = 2015}

@article{2017arXiv171101243G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171101243G},
	Archiveprefix = {arXiv},
	Author = {{Ghasemzadeh}, M. and {Samragh}, M. and {Koushanfar}, F.},
	Date-Added = {2018-04-28 01:41:17 +0000},
	Date-Modified = {2018-04-28 01:41:17 +0000},
	Eprint = {1711.01243},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	Month = nov,
	Primaryclass = {cs.LG},
	Title = {{ReBNet: Residual Binarized Neural Network}},
	Year = 2017}

@inproceedings{McDanel:2017:EBN:3108009.3108031,
	Acmid = {3108031},
	Address = {USA},
	Author = {McDanel, Bradley and Teerapittayanon, Surat and Kung, H.T.},
	Booktitle = {Proceedings of the 2017 International Conference on Embedded Wireless Systems and Networks},
	Date-Added = {2018-04-28 01:31:18 +0000},
	Date-Modified = {2018-04-28 01:34:56 +0000},
	Isbn = {978-0-9949886-1-4},
	Keywords = {BNN, eBNN, CNN, MNIST, CIFAR},
	Location = {Uppsala, Sweden},
	Numpages = {6},
	Pages = {168--173},
	Publisher = {Junction Publishing},
	Series = {EWSN \&\#8217;17},
	Title = {Embedded Binarized Neural Networks},
	Url = {http://dl.acm.org/citation.cfm?id=3108009.3108031},
	Year = {2017},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3108009.3108031}}

@article{2016arXiv160907061H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160907061H},
	Archiveprefix = {arXiv},
	Author = {{Hubara}, I. and {Courbariaux}, M. and {Soudry}, D. and {El-Yaniv}, R. and {Bengio}, Y.},
	Date-Added = {2018-04-27 00:08:19 +0000},
	Date-Modified = {2018-04-27 00:08:19 +0000},
	Eprint = {1609.07061},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	Month = sep,
	Title = {{Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations}},
	Year = 2016}

@inproceedings{Tang2017HowTT,
	Author = {Wei Tang and Gang Hua and Liang Wang},
	Booktitle = {AAAI},
	Date-Added = {2018-04-26 22:22:30 +0000},
	Date-Modified = {2018-04-26 22:22:52 +0000},
	Keywords = {BNN, NN, Training},
	Title = {How to Train a Compact Binary Neural Network with High Accuracy?},
	Year = {2017}}

@inproceedings{Wei:2017:ASA:3061639.3062207,
	Acmid = {3062207},
	Address = {New York, NY, USA},
	Articleno = {29},
	Author = {Wei, Xuechao and Yu, Cody Hao and Zhang, Peng and Chen, Youxiang and Wang, Yuxin and Hu, Han and Liang, Yun and Cong, Jason},
	Booktitle = {Proceedings of the 54th Annual Design Automation Conference 2017},
	Date-Added = {2018-04-25 23:12:26 +0000},
	Date-Modified = {2018-04-26 22:23:20 +0000},
	Doi = {10.1145/3061639.3062207},
	Isbn = {978-1-4503-4927-7},
	Keywords = {CNN, FPGA, Linear Algebra, Systolic Array},
	Location = {Austin, TX, USA},
	Numpages = {6},
	Pages = {29:1--29:6},
	Publisher = {ACM},
	Series = {DAC '17},
	Title = {Automated Systolic Array Architecture Synthesis for High Throughput CNN Inference on FPGAs},
	Url = {http://doi.acm.org/10.1145/3061639.3062207},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3061639.3062207},
	Bdsk-Url-2 = {https://doi.org/10.1145/3061639.3062207}}

@article{2016arXiv161207119U,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161207119U},
	Archiveprefix = {arXiv},
	Author = {{Umuroglu}, Y. and {Fraser}, N.~J. and {Gambardella}, G. and {Blott}, M. and {Leong}, P. and {Jahre}, M. and {Vissers}, K.},
	Date-Added = {2018-04-25 00:32:39 +0000},
	Date-Modified = {2018-04-25 00:32:39 +0000},
	Eprint = {1612.07119},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Hardware Architecture, Computer Science - Learning},
	Month = dec,
	Primaryclass = {cs.CV},
	Title = {{FINN: A Framework for Fast, Scalable Binarized Neural Network Inference}},
	Year = 2016}

@article{1998adap.org..6001P,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/1998adap.org..6001P},
	Author = {{Pang}, X. and {Werbos}, P.},
	Date-Added = {2018-04-24 23:59:42 +0000},
	Date-Modified = {2018-04-24 23:59:42 +0000},
	Journal = {Advances in Astrophysics},
	Keywords = {Adaptation, Noise, and Self-Organizing Systems, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Quantitative Biology - Neurons and Cognition},
	Month = jun,
	Title = {{Neural network design for J function approximation in dynamic programming}},
	Year = 1998}

@article{2018arXiv180200438Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180200438Z},
	Archiveprefix = {arXiv},
	Author = {{Zohouri}, H.~R. and {Podobas}, A. and {Matsuoka}, S.},
	Date-Added = {2018-04-24 23:57:57 +0000},
	Date-Modified = {2018-04-24 23:57:57 +0000},
	Eprint = {1802.00438},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	Month = feb,
	Primaryclass = {cs.DC},
	Title = {{Combined Spatial and Temporal Blocking for High-Performance Stencil Computation on FPGAs Using OpenCL}},
	Year = 2018}

@article{2017arXiv170703049M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170703049M},
	Archiveprefix = {arXiv},
	Author = {{Mostafa}, H. and {Pedroni}, B. and {Sheik}, S. and {Cauwenberghs}, G.},
	Date-Added = {2018-04-24 23:57:22 +0000},
	Date-Modified = {2018-04-24 23:57:22 +0000},
	Eprint = {1707.03049},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing},
	Month = jun,
	Title = {{Hardware-efficient on-line learning through pipelined truncated-error backpropagation in binary-state networks}},
	Year = 2017}

@article{2018arXiv180303790S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180303790S},
	Archiveprefix = {arXiv},
	Author = {{Shen}, J. and {Qiao}, Y. and {Huang}, Y. and {Wen}, M. and {Zhang}, C.},
	Date-Added = {2018-04-24 23:57:08 +0000},
	Date-Modified = {2018-04-24 23:57:08 +0000},
	Eprint = {1803.03790},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Hardware Architecture},
	Month = mar,
	Title = {{Towards a Multi-array Architecture for Accelerating Large-scale Matrix Multiplication on FPGAs}},
	Year = 2018}

@article{2016arXiv161200694H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161200694H},
	Archiveprefix = {arXiv},
	Author = {{Han}, S. and {Kang}, J. and {Mao}, H. and {Hu}, Y. and {Li}, X. and {Li}, Y. and {Xie}, D. and {Luo}, H. and {Yao}, S. and {Wang}, Y. and {Yang}, H. and {Dally}, W.~J.},
	Date-Added = {2018-04-24 10:49:59 +0000},
	Date-Modified = {2018-04-24 10:49:59 +0000},
	Eprint = {1612.00694},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computation and Language},
	Month = dec,
	Primaryclass = {cs.CL},
	Title = {{ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA}},
	Year = 2016}

@article{2011arXiv1107.1831H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2011arXiv1107.1831H},
	Archiveprefix = {arXiv},
	Author = {{Homescu}, C.},
	Date-Added = {2018-04-15 11:10:18 +0000},
	Date-Modified = {2018-04-15 11:10:18 +0000},
	Eprint = {1107.1831},
	Journal = {ArXiv e-prints},
	Keywords = {Quantitative Finance - Computational Finance},
	Month = jul,
	Primaryclass = {q-fin.CP},
	Title = {{Adjoints and Automatic (Algorithmic) Differentiation in Computational Finance}},
	Year = 2011}

@inproceedings{1188677,
	Author = {A. A. Gaffar and O. Mencer and W. Luk and P. Y. K. Cheung and N. Shirazi},
	Booktitle = {2002 IEEE International Conference on Field-Programmable Technology, 2002. (FPT). Proceedings.},
	Date-Added = {2018-04-15 10:59:42 +0000},
	Date-Modified = {2018-04-15 10:59:42 +0000},
	Doi = {10.1109/FPT.2002.1188677},
	Keywords = {FIR filters;VLSI;circuit CAD;circuit optimisation;data flow graphs;differentiation;digital filters;digital signal processing chips;discrete Fourier transforms;field programmable gate arrays;floating point arithmetic;high level synthesis;integrated circuit design;sensitivity analysis;DFT implementation;FIR filter implementation;FPGA;VLSI circuits;arithmetic operations;automatic bitwidth analysis;automatic differentiation;dataflow graph representation;discrete Fourier transform implementation;floating-point bitwidth analysis;floating-point designs;high-level programming;high-level synthesis;mathematical method;precision analysis;sensitivity analysis;user-defined numerical constraints;Arithmetic;Automatic programming;Circuits;Data analysis;Discrete Fourier transforms;Field programmable gate arrays;Finite impulse response filter;High level synthesis;Sensitivity analysis;Very large scale integration},
	Month = {Dec},
	Pages = {158-165},
	Title = {Floating-point bitwidth analysis via automatic differentiation},
	Year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1109/FPT.2002.1188677}}

@inproceedings{Elliott-2009-beautiful-differentiation,
	Author = {Conal Elliott},
	Date-Added = {2018-04-13 05:07:34 +0000},
	Date-Modified = {2018-04-13 05:07:34 +0000},
	Journal = {International Conference on Functional Programming (ICFP)},
	Keywords = {Automatic Differentiation, Vector Spaces, Haskell},
	Month = sep,
	Number = {ICFP},
	Title = {Beautiful Differentiation},
	Url = {http://conal.net/papers/beautiful-differentiation/},
	Year = {2009},
	Bdsk-Url-1 = {http://conal.net/papers/beautiful-differentiation/}}

@article{Elliott-2017-compiling-to-categories,
	Articleno = {48},
	Author = {Conal Elliott},
	Date-Added = {2018-04-07 11:10:44 +0000},
	Date-Modified = {2018-04-07 11:10:44 +0000},
	Doi = {http://dx.doi.org/10.1145/3110271},
	Journal = {Proc. ACM Program. Lang.},
	Keywords = {Automatic Differentiation, Category Theory, Haskell},
	Month = sep,
	Number = {ICFP},
	Numpages = {24},
	Title = {Compiling To Categories},
	Url = {http://conal.net/papers/compiling-to-categories},
	Volume = {1},
	Year = {2017},
	Bdsk-Url-1 = {http://conal.net/papers/compiling-to-categories},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/3110271}}

@article{Elliott:2018aa,
	Abstract = {Automatic differentiation (AD) in reverse mode (RAD) is a central component of deep learning and other uses of large-scale optimization. Commonly used RAD algorithms such as backpropagation, however, are complex and stateful, hindering deep understanding, improvement, and parallel execution. This paper develops a simple, generalized AD algorithm calculated from a simple, natural specification. The general algorithm can be specialized by varying the representation of derivatives. In particular, applying well-known constructions to a naive representation yields two RAD algorithms that are far simpler than previously known. In contrast to commonly used RAD implementations, the algorithms defined here involve no graphs, tapes, variables, partial derivatives, or mutation. They are inherently parallel-friendly, correct by construction, and usable directly from an existing programming language with no need for new data types or programming style, thanks to use of an AD-agnostic compiler plugin.
},
	Author = {Conal Elliott},
	Date-Added = {2018-04-06 11:42:18 +0000},
	Date-Modified = {2018-04-06 11:43:03 +0000},
	Eprint = {1804.00746},
	Keywords = {Automatic Differentiation, Math, Haskell, Category Theory},
	Month = {04},
	Title = {The simple essence of automatic differentiation (Differentiable functional programming made easy)},
	Url = {https://arxiv.org/pdf/1804.00746},
	Year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/abs/1804.00746},
	Bdsk-Url-2 = {https://arxiv.org/pdf/1804.00746}}

@article{Karczmarczuk2001,
	Abstract = {We present a purely functional implementation of the computational differentiation tools---the well known numeric (i.e., not symbolic) techniques which permit one to compute point-wise derivatives of functions defined by computer programs economically and exactly (with machine precision). We show how the use of lazy evaluation permits a transparent and elegant construction of the entire infinite tower of derivatives of higher order for any expressions present in the program. The formalism may be useful in various problems of scientific computing which often demand a hard and ungracious human preprocessing before writing the final code. Some concrete examples are given.},
	Author = {Karczmarczuk, Jerzy},
	Date-Added = {2018-04-06 04:46:21 +0000},
	Date-Modified = {2018-04-06 04:49:58 +0000},
	Day = {01},
	Doi = {10.1023/A:1011501232197},
	Issn = {1573-0557},
	Journal = {Higher-Order and Symbolic Computation},
	Keywords = {Mathematics, Derivates, Automatic Differentiation, Computer Science},
	Month = {Mar},
	Number = {1},
	Pages = {35--57},
	Title = {Functional Differentiation of Computer Programs},
	Url = {https://doi.org/10.1023/A:1011501232197},
	Volume = {14},
	Year = {2001},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1011501232197}}

@article{2014arXiv1404.7456G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1404.7456G},
	Archiveprefix = {arXiv},
	Author = {{Gunes Baydin}, A. and {Pearlmutter}, B.~A.},
	Date-Added = {2018-04-04 09:50:26 +0000},
	Date-Modified = {2018-04-04 09:50:26 +0000},
	Eprint = {1404.7456},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Symbolic Computation, Statistics - Machine Learning, 68W30, 65D25, 68T05, G.1.4, I.2.6},
	Month = apr,
	Primaryclass = {cs.LG},
	Title = {{Automatic Differentiation of Algorithms for Machine Learning}},
	Year = 2014}

@article{Gremse:2016aa,
	Abstract = {Many scientific problems such as classifier training or medical image reconstruction can be expressed as minimization of differentiable real-valued cost functions and solved with iterative gradient-based methods. Adjoint algorithmic differentiation (AAD) enables automated computation of gradients of such cost functions implemented as computer programs. To backpropagate adjoint derivatives, excessive memory is potentially required to store the intermediate partial derivatives on a dedicated data structure, referred to as the ``tape''. Parallelization is difficult because threads need to synchronize their accesses during taping and backpropagation. This situation is aggravated for many-core architectures, such as Graphics Processing Units (GPUs), because of the large number of light-weight threads and the limited memory size in general as well as per thread. We show how these limitations can be mediated if the cost function is expressed using GPU-accelerated vector and matrix operations which are recognized as intrinsic functions by our AAD software. We compare this approach with naive and vectorized implementations for CPUs. We use four increasingly complex cost functions to evaluate the performance with respect to memory consumption and gradient computation times. Using vectorization, CPU and GPU memory consumption could be substantially reduced compared to the naive reference implementation, in some cases even by an order of complexity. The vectorization allowed usage of optimized parallel libraries during forward and reverse passes which resulted in high speedups for the vectorized CPU version compared to the naive reference implementation. The GPU version achieved an additional speedup of 7.5 $\pm$4.4, showing that the processing power of GPUs can be utilized for AAD using this concept. Furthermore, we show how this software can be systematically extended for more complex problems such as nonlinear absorption reconstruction for fluorescence-mediated tomography.},
	An = {PMC4772124},
	Author = {Gremse, Felix and H{\"o}fter, Andreas and Razik, Lukas and Kiessling, Fabian and Naumann, Uwe},
	Date = {2016/03/01},
	Date-Added = {2018-04-04 08:10:03 +0000},
	Date-Modified = {2018-04-04 08:10:23 +0000},
	Db = {PMC},
	Doi = {10.1016/j.cpc.2015.10.027},
	Isbn = {0010-4655},
	J1 = {Comput Phys Commun},
	Journal = {Computer physics communications},
	Keywords = {GPU, Automatic Differentiation, Math, Neural Network},
	Month = {03},
	Pages = {300--311},
	Title = {GPU-Accelerated Adjoint Algorithmic Differentiation},
	Ty = {JOUR},
	U1 = {26941443{$[$}pmid{$]$}},
	Url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4772124/},
	Volume = {200},
	Year = {2016},
	Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4772124/},
	Bdsk-Url-2 = {https://dx.doi.org/10.1016/j.cpc.2015.10.027}}

@article{6701396,
	Author = {D. Neil and S. C. Liu},
	Date-Added = {2018-03-20 09:15:46 +0000},
	Date-Modified = {2018-03-20 09:15:46 +0000},
	Doi = {10.1109/TVLSI.2013.2294916},
	Issn = {1063-8210},
	Journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Keywords = {field programmable gate arrays;neural nets;CPU;MNIST handwritten digit classification;Minitaur;event-driven FPGA;event-driven neural network accelerator;field-programmable gate array-based system;neural networks;newsgroups classification data;robotics;spiking deep network;spiking network accelerator;Biological neural networks;Clocks;Computer architecture;Field programmable gate arrays;Mathematical model;Neurons;Performance evaluation;Deep belief networks;field programmable arrays;machine learning;neural networks;restricted Boltzmann machines;spiking neural networks},
	Month = {Dec},
	Number = {12},
	Pages = {2621-2628},
	Title = {Minitaur, an Event-Driven FPGA-Based Spiking Network Accelerator},
	Volume = {22},
	Year = {2014},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/TVLSI.2013.2294916}}

@book{Make-Your-Own-Neural-Network,
	Adsurl = {https://www.amazon.co.uk/Make-Your-Own-Neural-Network/dp/1530826608},
	Author = {{Rashid}, Tariq},
	Date-Added = {2018-03-03 10:33:43 +0000},
	Date-Modified = {2018-03-03 10:33:43 +0000},
	Keywords = {Computer Science, Neural Network, Python},
	Month = March,
	Title = {{Make Your Own Neural Network}},
	Year = 2016}

@article{798320,
	Author = {C. Elliott},
	Date-Added = {2018-02-26 09:50:52 +0000},
	Date-Modified = {2018-02-26 09:50:52 +0000},
	Doi = {10.1109/32.798320},
	Issn = {0098-5589},
	Journal = {IEEE Transactions on Software Engineering},
	Keywords = {computer animation;multimedia computing;simulation languages;Fran;Haskell;declarative host language;embedded domain-specific vocabulary;embedded modeling language approach;growth;interactive 3D animation;interactive multimedia animation;modeled animation;motion;Animation;Automatic programming;Computer graphics;Computer languages;Domain specific languages;Functional programming;Programming profession;Shape;Vocabulary;Writing},
	Month = {May},
	Number = {3},
	Pages = {291-308},
	Title = {An embedded modeling language approach to interactive 3D and multimedia animation},
	Volume = {25},
	Year = {1999},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/32.798320}}

@techreport{Hudak94vs.ada,
	Author = {Paul Hudak and Mark P. Jones},
	Date-Added = {2018-02-26 09:38:35 +0000},
	Date-Modified = {2018-02-26 09:38:35 +0000},
	Keywords = {DARPA, haskell, Cpp, Awk},
	Title = {vs. Ada vs. C++ vs. Awk vs. ... An Experiment in Software Prototyping Productivity Available from http://www.haskell.org/papers/NSWC/jfp.ps},
	Year = {1994}}

@inproceedings{Totoo:2012:HVF:2364474.2364483,
	Acmid = {2364483},
	Address = {New York, NY, USA},
	Author = {Totoo, Prabhat and Deligiannis, Pantazis and Loidl, Hans-Wolfgang},
	Booktitle = {Proceedings of the 1st ACM SIGPLAN Workshop on Functional High-performance Computing},
	Date-Added = {2018-02-26 08:51:00 +0000},
	Date-Modified = {2018-02-26 08:51:00 +0000},
	Doi = {10.1145/2364474.2364483},
	Isbn = {978-1-4503-1577-7},
	Keywords = {barnes-hut, f\#, haskell, n-body, parallelism, scala},
	Location = {Copenhagen, Denmark},
	Numpages = {12},
	Pages = {49--60},
	Publisher = {ACM},
	Series = {FHPC '12},
	Title = {Haskell vs. F\# vs. Scala: A High-level Language Features and Parallelism Support Comparison},
	Url = {http://doi.acm.org/10.1145/2364474.2364483},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2364474.2364483},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2364474.2364483}}

@inproceedings{6209130,
	Author = {M. Fenwick and C. Sesanker and M. R. Schiller and H. J. Ellis and M. L. Hinman and J. Vyas and M. R. Gryk},
	Booktitle = {2012 Ninth International Conference on Information Technology - New Generations},
	Date-Added = {2018-02-26 08:28:41 +0000},
	Date-Modified = {2018-02-26 08:28:41 +0000},
	Doi = {10.1109/ITNG.2012.21},
	Keywords = {Java;LISP;bioinformatics;functional programming;learning (artificial intelligence);public domain software;software engineering;Haskell;Java;LISP;Python;algorithm development;bioinformatics;complex data operations;complex mathematical notions;data processing;functional computing;functional languages;functional programming accessibility;functional programming techniques;learning curve;learning resources;machine learning;multilanguage source-code repository;open-source Sandbox;scientific communities;software integration;Bioinformatics;Data visualization;Functional programming;Nuclear magnetic resonance;Proteins;Schedules;Transient analysis;Clojure;Haskell;Java;LISP;NMR;bioinformatics;functional-programming},
	Month = {April},
	Pages = {89-94},
	Title = {An Open-Source Sandbox for Increasing the Accessibility of Functional Programming to the Bioinformatics and Scientific Communities},
	Year = {2012},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8Qay4uL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0RDR0FOL0dhbmcgb2YgR0FOcy0gR2VuZXJhdGl2ZSBBZHZlcnNhcmlhbCBOZXR3b3JrcyB3aXRoIE1heGltdW0gTWFyZ2luIFJhbmtpbmcuYmli0hcLGBlXTlMuZGF0YU8RAnIAAAAAAnIAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9HYW5nIG9mIEdBTnMtIEdlbmUjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAFRENHQU4AAAIAgS86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpEQ0dBTjpHYW5nIG9mIEdBTnMtIEdlbmVyYXRpdmUgQWR2ZXJzYXJpYWwgTmV0d29ya3Mgd2l0aCBNYXhpbXVtIE1hcmdpbiBSYW5raW5nLmJpYgAADgCcAE0ARwBhAG4AZwAgAG8AZgAgAEcAQQBOAHMALQAgAEcAZQBuAGUAcgBhAHQAaQB2AGUAIABBAGQAdgBlAHIAcwBhAHIAaQBhAGwAIABOAGUAdAB3AG8AcgBrAHMAIAB3AGkAdABoACAATQBhAHgAaQBtAHUAbQAgAE0AYQByAGcAaQBuACAAUgBhAG4AawBpAG4AZwAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAf1VzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRENHQU4vR2FuZyBvZiBHQU5zLSBHZW5lcmF0aXZlIEFkdmVyc2FyaWFsIE5ldHdvcmtzIHdpdGggTWF4aW11bSBNYXJnaW4gUmFua2luZy5iaWIAABMAAS8AABUAAgAL//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4A/AEBAQkDfwOBA4YDkQOaA6gDrAOzA7wDwQPOA9ED4wPmA+sAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAD7Q==},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ITNG.2012.21}}

@inproceedings{Pop:2010:ERH:1863543.1863595,
	Acmid = {1863595},
	Address = {New York, NY, USA},
	Author = {Pop, Iustin},
	Booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
	Date-Added = {2018-02-26 08:27:07 +0000},
	Date-Modified = {2018-02-26 08:27:07 +0000},
	Doi = {10.1145/1863543.1863595},
	Isbn = {978-1-60558-794-3},
	Keywords = {ganeti, haskell, python, system administration},
	Location = {Baltimore, Maryland, USA},
	Numpages = {6},
	Pages = {369--374},
	Publisher = {ACM},
	Series = {ICFP '10},
	Title = {Experience Report: Haskell As a Reagent: Results and Observations on the Use of Haskell in a Python Project},
	Url = {http://doi.acm.org/10.1145/1863543.1863595},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1863543.1863595},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1863543.1863595}}

@inproceedings{Nanz:2015:CSP:2818754.2818848,
	Acmid = {2818848},
	Address = {Piscataway, NJ, USA},
	Author = {Nanz, Sebastian and Furia, Carlo A.},
	Booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
	Date-Added = {2018-02-26 07:03:13 +0000},
	Date-Modified = {2018-04-15 11:11:54 +0000},
	Isbn = {978-1-4799-1934-5},
	Keywords = {Languages, Haskell, C++},
	Location = {Florence, Italy},
	Numpages = {11},
	Pages = {778--788},
	Publisher = {IEEE Press},
	Series = {ICSE '15},
	Title = {A Comparative Study of Programming Languages in Rosetta Code},
	Url = {http://dl.acm.org/citation.cfm?id=2818754.2818848},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2818754.2818848}}

@article{2010arXiv1009.0305R,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2010arXiv1009.0305R},
	Archiveprefix = {arXiv},
	Author = {{Rabah}, S. and {Li}, J. and {Liu}, M. and {Lai}, Y.},
	Date-Added = {2018-02-22 23:19:52 +0000},
	Date-Modified = {2018-02-22 23:19:52 +0000},
	Eprint = {1009.0305},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Programming Languages, D.3},
	Month = sep,
	Primaryclass = {cs.PL},
	Title = {{Comparative Studies of 10 Programming Languages within 10 Diverse Criteria -- a Team 7 COMP6411-S10 Term Report}},
	Year = 2010}

@article{2017arXiv171109846J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171109846J},
	Archiveprefix = {arXiv},
	Author = {{Jaderberg}, M. and {Dalibard}, V. and {Osindero}, S. and {Czarnecki}, W.~M. and {Donahue}, J. and {Razavi}, A. and {Vinyals}, O. and {Green}, T. and {Dunning}, I. and {Simonyan}, K. and {Fernando}, C. and {Kavukcuoglu}, K.},
	Date-Added = {2018-02-21 06:49:04 +0000},
	Date-Modified = {2018-02-21 06:49:04 +0000},
	Eprint = {1711.09846},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = nov,
	Primaryclass = {cs.LG},
	Title = {{Population Based Training of Neural Networks}},
	Year = 2017}

@article{2014arXiv1410.5401G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1410.5401G},
	Archiveprefix = {arXiv},
	Author = {{Graves}, A. and {Wayne}, G. and {Danihelka}, I.},
	Date-Added = {2018-02-21 06:07:10 +0000},
	Date-Modified = {2018-02-21 06:07:10 +0000},
	Eprint = {1410.5401},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing},
	Month = oct,
	Title = {{Neural Turing Machines}},
	Year = 2014}

@article{2015arXiv150308895S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150308895S},
	Archiveprefix = {arXiv},
	Author = {{Sukhbaatar}, S. and {Szlam}, A. and {Weston}, J. and {Fergus}, R.},
	Date-Added = {2018-02-21 05:45:01 +0000},
	Date-Modified = {2018-02-21 05:45:01 +0000},
	Eprint = {1503.08895},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	Month = mar,
	Title = {{End-To-End Memory Networks}},
	Year = 2015}

@article{2014arXiv1409.0473B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1409.0473B},
	Archiveprefix = {arXiv},
	Author = {{Bahdanau}, D. and {Cho}, K. and {Bengio}, Y.},
	Date-Added = {2018-02-21 05:29:04 +0000},
	Date-Modified = {2018-02-21 05:29:04 +0000},
	Eprint = {1409.0473},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	Month = sep,
	Primaryclass = {cs.CL},
	Title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
	Year = 2014}

@article{2015arXiv150203044X,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150203044X},
	Archiveprefix = {arXiv},
	Author = {{Xu}, K. and {Ba}, J. and {Kiros}, R. and {Cho}, K. and {Courville}, A. and {Salakhutdinov}, R. and {Zemel}, R. and {Bengio}, Y.},
	Date-Added = {2018-02-21 04:55:23 +0000},
	Date-Modified = {2018-02-21 04:55:23 +0000},
	Eprint = {1502.03044},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	Month = feb,
	Primaryclass = {cs.LG},
	Title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
	Year = 2015}

@article{2015arXiv150301007J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150301007J},
	Archiveprefix = {arXiv},
	Author = {{Joulin}, A. and {Mikolov}, T.},
	Date-Added = {2018-02-21 04:03:18 +0000},
	Date-Modified = {2018-02-21 04:03:18 +0000},
	Eprint = {1503.01007},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	Month = mar,
	Title = {{Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets}},
	Year = 2015}

@article{2014arXiv1406.6247M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1406.6247M},
	Archiveprefix = {arXiv},
	Author = {{Mnih}, V. and {Heess}, N. and {Graves}, A. and {Kavukcuoglu}, K.},
	Date-Added = {2018-02-21 04:01:10 +0000},
	Date-Modified = {2018-02-21 04:01:10 +0000},
	Eprint = {1406.6247},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{Recurrent Models of Visual Attention}},
	Year = 2014}

@article{2013arXiv1308.0850G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1308.0850G},
	Archiveprefix = {arXiv},
	Author = {{Graves}, A.},
	Date-Added = {2018-02-21 03:44:05 +0000},
	Date-Modified = {2018-02-21 03:44:05 +0000},
	Eprint = {1308.0850},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	Month = aug,
	Title = {{Generating Sequences With Recurrent Neural Networks}},
	Year = 2013}

@article{2015arXiv150204390D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150204390D},
	Archiveprefix = {arXiv},
	Author = {{Dauphin}, Y.~N. and {de Vries}, H. and {Bengio}, Y.},
	Date-Added = {2018-02-21 02:45:02 +0000},
	Date-Modified = {2018-02-21 02:45:02 +0000},
	Eprint = {1502.04390},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Numerical Analysis},
	Month = feb,
	Primaryclass = {cs.LG},
	Title = {{Equilibrated adaptive learning rates for non-convex optimization}},
	Year = 2015}

@article{2016arXiv160604934K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160604934K},
	Archiveprefix = {arXiv},
	Author = {{Kingma}, D.~P. and {Salimans}, T. and {Jozefowicz}, R. and {Chen}, X. and {Sutskever}, I. and {Welling}, M.},
	Date-Added = {2018-02-20 10:42:29 +0000},
	Date-Modified = {2018-02-20 10:42:29 +0000},
	Eprint = {1606.04934},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{Improving Variational Inference with Inverse Autoregressive Flow}},
	Year = 2016}

@article{2015arXiv150204623G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150204623G},
	Archiveprefix = {arXiv},
	Author = {{Gregor}, K. and {Danihelka}, I. and {Graves}, A. and {Jimenez Rezende}, D. and {Wierstra}, D.},
	Date-Added = {2018-02-20 10:42:05 +0000},
	Date-Modified = {2018-02-20 10:42:05 +0000},
	Eprint = {1502.04623},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = feb,
	Primaryclass = {cs.CV},
	Title = {{DRAW: A Recurrent Neural Network For Image Generation}},
	Year = 2015}

@article{2016arXiv160603657C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160603657C},
	Archiveprefix = {arXiv},
	Author = {{Chen}, X. and {Duan}, Y. and {Houthooft}, R. and {Schulman}, J. and {Sutskever}, I. and {Abbeel}, P.},
	Date-Added = {2018-02-20 00:17:44 +0000},
	Date-Modified = {2018-02-20 00:17:44 +0000},
	Eprint = {1606.03657},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
	Year = 2016}

@article{2016arXiv160509674H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160509674H},
	Archiveprefix = {arXiv},
	Author = {{Houthooft}, R. and {Chen}, X. and {Duan}, Y. and {Schulman}, J. and {De Turck}, F. and {Abbeel}, P.},
	Date-Added = {2018-02-19 23:51:31 +0000},
	Date-Modified = {2018-02-19 23:51:31 +0000},
	Eprint = {1605.09674},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Machine Learning},
	Month = may,
	Primaryclass = {cs.LG},
	Title = {{VIME: Variational Information Maximizing Exploration}},
	Year = 2016}

@article{2016arXiv160603476H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160603476H},
	Archiveprefix = {arXiv},
	Author = {{Ho}, J. and {Ermon}, S.},
	Date-Added = {2018-02-19 10:11:33 +0000},
	Date-Modified = {2018-02-19 10:11:33 +0000},
	Eprint = {1606.03476},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{Generative Adversarial Imitation Learning}},
	Year = 2016}

@article{DBLP:journals/corr/SalimansGZCRC16,
	Archiveprefix = {arXiv},
	Author = {Tim Salimans and Ian J. Goodfellow and Wojciech Zaremba and Vicki Cheung and Alec Radford and Xi Chen},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.org/rec/bib/journals/corr/SalimansGZCRC16},
	Date-Added = {2018-02-19 09:53:01 +0000},
	Date-Modified = {2018-02-19 10:06:38 +0000},
	Eprint = {1606.03498},
	Journal = {CoRR},
	Keywords = {DCGAN, MNIST, Semi-Supervised learning, minibatch},
	Timestamp = {Wed, 07 Jun 2017 14:40:52 +0200},
	Title = {Improved Techniques for Training GANs},
	Url = {http://arxiv.org/abs/1606.03498},
	Volume = {abs/1606.03498},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1606.03498}}

@article{2014arXiv1406.2661G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1406.2661G},
	Archiveprefix = {arXiv},
	Author = {{Goodfellow}, I.~J. and {Pouget-Abadie}, J. and {Mirza}, M. and {Xu}, B. and {Warde-Farley}, D. and {Ozair}, S. and {Courville}, A. and {Bengio}, Y.},
	Date-Added = {2018-02-19 09:28:01 +0000},
	Date-Modified = {2018-02-19 09:28:01 +0000},
	Eprint = {1406.2661},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Learning},
	Month = jun,
	Primaryclass = {stat.ML},
	Title = {{Generative Adversarial Networks}},
	Year = 2014}

@article{2013arXiv1312.6114K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1312.6114K},
	Archiveprefix = {arXiv},
	Author = {{Kingma}, D.~P and {Welling}, M.},
	Date-Added = {2018-02-19 09:25:50 +0000},
	Date-Modified = {2018-02-19 09:25:50 +0000},
	Eprint = {1312.6114},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Learning},
	Month = dec,
	Primaryclass = {stat.ML},
	Title = {{Auto-Encoding Variational Bayes}},
	Year = 2013}

@article{DBLP:journals/corr/OordKK16,
	Archiveprefix = {arXiv},
	Author = {A{\"{a}}ron van den Oord and Nal Kalchbrenner and Koray Kavukcuoglu},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.org/rec/bib/journals/corr/OordKK16},
	Date-Added = {2018-02-19 09:01:51 +0000},
	Date-Modified = {2018-02-19 09:05:45 +0000},
	Eprint = {1601.06759},
	Journal = {CoRR},
	Keywords = {RNN, PixelRNN, BiLSTM, MNIST},
	Timestamp = {Wed, 07 Jun 2017 14:40:22 +0200},
	Title = {Pixel Recurrent Neural Networks},
	Url = {http://arxiv.org/abs/1601.06759},
	Volume = {abs/1601.06759},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1601.06759}}

@article{DBLP:journals/corr/KulkarniWKT15,
	Archiveprefix = {arXiv},
	Author = {Tejas D. Kulkarni and Will Whitney and Pushmeet Kohli and Joshua B. Tenenbaum},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.org/rec/bib/journals/corr/KulkarniWKT15},
	Date-Added = {2018-02-19 04:44:04 +0000},
	Date-Modified = {2018-02-19 04:44:43 +0000},
	Eprint = {1503.03167},
	Journal = {CoRR},
	Keywords = {CNN, IGN, SGVB, DC-IGN},
	Timestamp = {Wed, 07 Jun 2017 14:40:29 +0200},
	Title = {Deep Convolutional Inverse Graphics Network},
	Url = {http://arxiv.org/abs/1503.03167},
	Volume = {abs/1503.03167},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1503.03167}}

@article{2017arXiv170100160G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170100160G},
	Archiveprefix = {arXiv},
	Author = {{Goodfellow}, I.},
	Date-Added = {2018-02-16 06:51:27 +0000},
	Date-Modified = {2018-02-16 06:51:27 +0000},
	Eprint = {1701.00160},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning},
	Month = dec,
	Primaryclass = {cs.LG},
	Title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
	Year = 2017}

@article{2017arXiv170900199H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170900199H},
	Archiveprefix = {arXiv},
	Author = {{Hadad}, N. and {Wolf}, L. and {Shahar}, M.},
	Date-Added = {2018-02-07 01:32:29 +0000},
	Date-Modified = {2018-02-07 01:32:29 +0000},
	Eprint = {1709.00199},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = sep,
	Primaryclass = {cs.LG},
	Title = {{Two-Step Disentanglement for Financial Data}},
	Year = 2017}

@inproceedings{10.1007/3-540-28438-9_2,
	Abstract = {Backwards calculation of derivatives -- sometimes called the reverse mode, the full adjoint method, or backpropagation -- has been developed and applied in many fields. This paper reviews several strands of history, advanced capabilities and types of application -- particularly those which are crucial to the development of brain-like capabilities in intelligent control and artificial intelligence.},
	Address = {Berlin, Heidelberg},
	Author = {Werbos, Paul J.},
	Booktitle = {Automatic Differentiation: Applications, Theory, and Implementations},
	Date-Added = {2018-02-07 01:11:34 +0000},
	Date-Modified = {2018-04-15 11:11:31 +0000},
	Editor = {B{\"u}cker, Martin and Corliss, George and Naumann, Uwe and Hovland, Paul and Norris, Boyana},
	Isbn = {978-3-540-28438-3},
	Keywords = {NN, Backpropagation},
	Pages = {15--34},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Backwards Differentiation in AD and Neural Nets: Past Links and New Opportunities},
	Year = {2006}}

@article{2014arXiv1404.7828S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1404.7828S},
	Archiveprefix = {arXiv},
	Author = {{Schmidhuber}, J.},
	Date-Added = {2018-02-07 00:15:56 +0000},
	Date-Modified = {2018-02-07 00:15:56 +0000},
	Eprint = {1404.7828},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	Month = apr,
	Title = {{Deep Learning in Neural Networks: An Overview}},
	Year = 2014}

@article{2017arXiv170107274L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170107274L},
	Archiveprefix = {arXiv},
	Author = {{Li}, Y.},
	Date-Added = {2018-02-06 09:19:00 +0000},
	Date-Modified = {2018-02-06 09:19:00 +0000},
	Eprint = {1701.07274},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning},
	Month = jan,
	Primaryclass = {cs.LG},
	Title = {{Deep Reinforcement Learning: An Overview}},
	Year = 2017}

@incollection{NIPS2017_6917,
	Author = {XIAO, SHUAI and Farajtabar, Mehrdad and Ye, Xiaojing and Yan, Junchi and Yang, Xiaokang and Song, Le and Zha, Hongyuan},
	Booktitle = {Advances in Neural Information Processing Systems 30},
	Date-Added = {2018-02-06 09:12:15 +0000},
	Date-Modified = {2018-02-06 09:13:20 +0000},
	Editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	Keywords = {DNN, Generative Adversarial NN, Point processes},
	Pages = {3250--3259},
	Publisher = {Curran Associates, Inc.},
	Title = {Wasserstein Learning of Deep Generative Point Process Models},
	Url = {http://papers.nips.cc/paper/6917-wasserstein-learning-of-deep-generative-point-process-models.pdf},
	Year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6917-wasserstein-learning-of-deep-generative-point-process-models.pdf}}

@book{TheWayOfTheTurtle,
	Author = {Curtis M. Faith},
	Date-Added = {2018-02-06 08:38:52 +0000},
	Date-Modified = {2018-02-06 08:40:50 +0000},
	Keywords = {Finance, Economics Personal, Professional Development},
	Month = {March},
	Number = {9780071486644},
	Publisher = {McGraw-Hill Osborne Media},
	Title = {Way of the Turtle},
	Year = {2007}}

@article{2015arXiv150205767G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150205767G},
	Archiveprefix = {arXiv},
	Author = {{Gunes Baydin}, A. and {Pearlmutter}, B.~A. and {Andreyevich Radul}, A. and {Siskind}, J.~M.},
	Date-Added = {2018-02-06 01:01:08 +0000},
	Date-Modified = {2018-02-06 01:01:08 +0000},
	Eprint = {1502.05767},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Symbolic Computation, Computer Science - Learning, 68W30, 65D25, 68T05, G.1.4, I.2.6},
	Month = feb,
	Primaryclass = {cs.SC},
	Title = {{Automatic differentiation in machine learning: a survey}},
	Year = 2015}

@misc{2015arXiv151106434R-sc,
	Author = {{Radford}, A. and {Metz}, L. and {Chintala}, S.},
	Date-Added = {2018-02-05 09:50:12 +0000},
	Date-Modified = {2018-02-23 00:38:59 +0000},
	Keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	Month = nov,
	Title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
	Url = {https://github.com/Newmu/dcgan_code},
	Year = 2015,
	Bdsk-Url-1 = {https://github.com/Newmu/dcgan_code}}

@article{2015arXiv151106434R,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151106434R},
	Archiveprefix = {arXiv},
	Author = {{Radford}, A. and {Metz}, L. and {Chintala}, S.},
	Date-Added = {2018-02-05 09:50:12 +0000},
	Date-Modified = {2018-02-05 09:50:12 +0000},
	Eprint = {1511.06434},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	Month = nov,
	Primaryclass = {cs.LG},
	Title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
	Year = 2015}

@misc{Grenade-Github-Code,
	Address = {San Francisco, CA, USA},
	Author = {Campbell, Huw},
	Date-Added = {2018-02-05 09:27:10 +0000},
	Date-Modified = {2018-02-23 00:39:26 +0000},
	Day = {24},
	Keywords = {functional programming, DNN, CNN, sourcecode},
	Month = {June},
	Publisher = {Github},
	Title = {Grenade},
	Url = {https://github.com/HuwCampbell/grenade},
	Year = {2016},
	Bdsk-Url-1 = {https://github.com/HuwCampbell/grenade}}

@inproceedings{Zhang:2015:OFA:2684746.2689060,
	Acmid = {2689060},
	Address = {New York, NY, USA},
	Author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason},
	Booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-02-05 09:06:58 +0000},
	Date-Modified = {2018-02-05 09:06:58 +0000},
	Doi = {10.1145/2684746.2689060},
	Isbn = {978-1-4503-3315-3},
	Keywords = {acceleration, convolutional neural network, fpga, roofline model},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {161--170},
	Publisher = {ACM},
	Series = {FPGA '15},
	Title = {Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks},
	Url = {http://doi.acm.org/10.1145/2684746.2689060},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2684746.2689060},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2684746.2689060}}

@article{2016arXiv161102450W,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161102450W},
	Archiveprefix = {arXiv},
	Author = {{Wang}, D. and {An}, J. and {Xu}, K.},
	Date-Added = {2018-02-05 09:06:58 +0000},
	Date-Modified = {2018-02-05 09:06:58 +0000},
	Eprint = {1611.02450},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Hardware Architecture},
	Month = nov,
	Title = {{PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks}},
	Year = 2016}

@article{anonymous2018wavelet,
	Author = {Anonymous},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Journal = {International Conference on Learning Representations},
	Keywords = {Wavelet, CNN, MIST, MathConvNet},
	Title = {Wavelet Pooling for Convolutional Neural Networks},
	Url = {https://openreview.net/forum?id=rkhlb8lCZ},
	Year = {2018},
	Bdsk-Url-1 = {https://openreview.net/forum?id=rkhlb8lCZ}}

@book{Nayak:2017aa,
	Author = {Nayak, Sarat and Bihari Misra, Bijan and Behera, Dr. H.},
	Date = {2017/01/01},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.4018/978-1-5225-0788-8.ch022},
	Journal = {Nature-Inspired Computing: Concepts, Methodologies, Tools, and Applications},
	Keywords = {HONN, Pi-Sigma, Genetic Algorithms},
	Month = {01},
	N2 = {This chapter presents two higher order neural networks (HONN) for efficient prediction of stock market behavior. The models include Pi-Sigma, and Sigma-Pi higher order neural network models. Along with the traditional gradient descent learning, how the evolutionary computation technique such as genetic algorithm (GA) can be used effectively for the learning process is also discussed here. The learning process is made adaptive to handle the noise and uncertainties associated with stock market data. Further, different prediction approaches are discussed here and application of HONN for time series forecasting is illustrated with real life data taken from a number of stock markets across the globe.},
	Title = {Adaptive Hybrid Higher Order Neural Networks for Prediction of Stock Market Behavior},
	Ty = {BOOK},
	Year = {2017},
	Bdsk-Url-1 = {https://dx.doi.org/10.4018/978-1-5225-0788-8.ch022}}

@article{CHONG2017187,
	Abstract = {We offer a systematic analysis of the use of deep learning networks for stock market analysis and prediction. Its ability to extract features from a large set of raw data without relying on prior knowledge of predictors makes deep learning potentially attractive for stock market prediction at high frequencies. Deep learning algorithms vary considerably in the choice of network structure, activation function, and other model parameters, and their performance is known to depend heavily on the method of data representation. Our study attempts to provides a comprehensive and objective assessment of both the advantages and drawbacks of deep learning algorithms for stock market analysis and prediction. Using high-frequency intraday stock returns as input data, we examine the effects of three unsupervised feature extraction methods---principal component analysis, autoencoder, and the restricted Boltzmann machine---on the network's overall ability to predict future market behavior. Empirical results suggest that deep neural networks can extract additional information from the residuals of the autoregressive model and improve prediction performance; the same cannot be said when the autoregressive model is applied to the residuals of the network. Covariance estimation is also noticeably improved when the predictive network is applied to covariance-based market structure analysis. Our study offers practical insights and potentially useful directions for further investigation into how deep learning networks can be effectively used for stock market analysis and prediction.},
	Author = {Eunsuk Chong and Chulwoo Han and Frank C. Park},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {https://doi.org/10.1016/j.eswa.2017.04.030},
	Issn = {0957-4174},
	Journal = {Expert Systems with Applications},
	Keywords = {Stock market prediction, Deep learning, Multilayer neural network, Covariance estimation},
	Pages = {187 - 205},
	Title = {Deep learning networks for stock market analysis and prediction: Methodology, data representations, and case studies},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957417417302750},
	Volume = {83},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0957417417302750},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.eswa.2017.04.030}}

@inproceedings{HuangY.2016Etmt,
	Author = {Huang, Y. and Huang, K. and Wang, Y. and Zhang, H. and Guan, J. and Zhou, S.},
	Copyright = {Copyright 2017 Elsevier B.V., All rights reserved.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Isbn = {9783319422961},
	Issn = {03029743},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Keywords = {Convolutional Neural Network ; Deep Neural Network ; Financial Trend Prediction ; Twitter Mood},
	Pages = {449--460},
	Publisher = {Springer Verlag},
	Title = {Exploiting twitter moods to boost financial trend prediction based on deep network models},
	Volume = {9773},
	Year = {2016}}

@article{diartificial,
	Author = {Di Persio, Luca and Honchar, Oleksandr},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Journal = {International Journal of Economics and Management Systems},
	Keywords = {CNN, RNN, LSTM},
	Month = {January},
	Pages = {5},
	Title = {Artificial neural networks approach to the forecast of stock market price movements},
	Volume = {1},
	Year = {2016}}

@techreport{ghoshal2017reading,
	Author = {Ghoshal, Sid and Roberts, Stephen},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Institution = {Technical report},
	Keywords = {CNN, RNN, Technical Analysis},
	Title = {Reading the Tea Leaves: A Neural Network Perspective on Technical Trading},
	Year = {2017}}

@article{aggarwal2017deep,
	Author = {Aggarwal, Saurabh and Aggarwal, Somya},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Journal = {International Journal of Computer Applications},
	Keywords = {DNN, Finance, Portfolio, LSTM},
	Number = {2},
	Publisher = {Foundation of Computer Science},
	Title = {Deep Investment in Financial Markets using Deep Learning Models},
	Volume = {162},
	Year = {2017}}

@article{di2016artificial,
	Author = {Di Persio, Luca and Honchar, Oleksandr},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Journal = {International Journal of Circuits, Systems and Signal Processing},
	Keywords = {CNN, MLP, LSTM, Wavelet},
	Pages = {403--413},
	Title = {Artificial Neural Networks architectures for stock price prediction: comparisons and applications},
	Volume = {10},
	Year = {2016}}

@article{dixon2016classification,
	Author = {Dixon, Matthew Francis and Klabjan, Diego and Bang, Jin Hoon},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {DNN, Futures, Finance},
	Title = {Classification-based Financial Markets Prediction using Deep Neural Networks},
	Year = {2016}}

@misc{essay59381,
	Author = {M. {Kooijman}},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {Haskell, functional programming},
	Month = {December},
	Title = {Haskell as a higher order structural hardware description language},
	Url = {http://essay.utwente.nl/59381/},
	Year = {2009},
	Bdsk-Url-1 = {http://essay.utwente.nl/59381/}}

@conference{L:08,
	Author = {Philip H.W. Leong},
	Booktitle = {Proc. 4th IEEE International Symposium on Electronic Design, Test and Applications},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {FPGA, Trends},
	Location = {Hong Kong},
	Note = {\textbf{Invited}},
	Pages = {137--141},
	Title = {Recent Trends in {FPGA} Architectures and Applications},
	Url = {rtfpga_delta08.pdf},
	Year = {2008},
	Bdsk-Url-1 = {rtfpga_delta08.pdf}}

@inproceedings{Pike:2009:RYO:1596638.1596646,
	Acmid = {1596646},
	Address = {New York, NY, USA},
	Author = {Pike, Lee and Brown, Geoffrey and Goodloe, Alwyn},
	Booktitle = {Proceedings of the 2Nd ACM SIGPLAN Symposium on Haskell},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/1596638.1596646},
	Isbn = {978-1-60558-508-6},
	Keywords = {emulation, functional programming, physical-layer protocol testing},
	Location = {Edinburgh, Scotland},
	Numpages = {8},
	Pages = {61--68},
	Publisher = {ACM},
	Series = {Haskell '09},
	Title = {Roll Your Own Test Bed for Embedded Real-time Protocols: A Haskell Experience},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1596638.1596646},
	Year = {2009},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QOS4uL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OL0VuZC1Uby1FbmQgTWVtb3J5IE5ldHdvcmtzLmJpYtIXCxgZV05TLmRhdGFPEQGsAAAAAAGsAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8eRW5kLVRvLUVuZCBNZW1vcnkgTmV0d29ya3MuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAAAk5OAAIATy86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpOTjpFbmQtVG8tRW5kIE1lbW9yeSBOZXR3b3Jrcy5iaWIAAA4APgAeAEUAbgBkAC0AVABvAC0ARQBuAGQAIABNAGUAbQBvAHIAeQAgAE4AZQB0AHcAbwByAGsAcwAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIATVVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vRW5kLVRvLUVuZCBNZW1vcnkgTmV0d29ya3MuYmliAAATAAEvAAAVAAIAC///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAMoAzwDXAocCiQKOApkCogKwArQCuwLEAskC1gLZAusC7gLzAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAvU=},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1596638.1596646},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1596638.1596646}}

@inproceedings{Schrage:2005:HRD:1088348.1088351,
	Acmid = {1088351},
	Address = {New York, NY, USA},
	Author = {Schrage, Martijn M. and van IJzendoorn, Arjan and van der Gaag, Linda C.},
	Booktitle = {Proceedings of the 2005 ACM SIGPLAN Workshop on Haskell},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/1088348.1088351},
	Isbn = {1-59593-071-X},
	Keywords = {application, bayesian networks, graphical user interface, haskell, wxHaskell},
	Location = {Tallinn, Estonia},
	Numpages = {10},
	Pages = {17--26},
	Publisher = {ACM},
	Series = {Haskell '05},
	Title = {Haskell Ready to Dazzle the Real World},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1088348.1088351},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1088348.1088351},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1088348.1088351}}

@article{Sezer:2017aa,
	Author = {Sezer, Omer Berat and Ozbayoglu, Murat and Dogdu, Erdogan},
	Booktitle = {Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical Systems, CAS October 30 --November 1, 2017, Chicago, Illinois, USA},
	Da = {2017/01/01/},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {https://doi.org/10.1016/j.procs.2017.09.031},
	Isbn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {Stock Trading; Stock Market; Deep Neural-Network; Evolutionary Algorithms; Technical Analysis},
	Number = {Supplement C},
	Pages = {473--480},
	Title = {A Deep Neural-Network Based Stock Trading System Based on Evolutionary Optimized Technical Analysis Parameters},
	Ty = {JOUR},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050917318252},
	Volume = {114},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050917318252},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2017.09.031}}

@inproceedings{Kablan:2009aa,
	Author = {A. Kablan},
	Booktitle = {2009 Third International Conference on Advanced Engineering Computing and Applications in Sciences},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/ADVCOMP.2009.23},
	Journal = {2009 Third International Conference on Advanced Engineering Computing and Applications in Sciences},
	Journal1 = {2009 Third International Conference on Advanced Engineering Computing and Applications in Sciences},
	Keywords = {decision making; expert systems; financial data processing; fuzzy reasoning; neural nets; pattern recognition; stock markets; adaptive neuro fuzzy inference systems; automated trading strategy; decision making; efficient market hypothesis; expert system; financial forecasting; financial markets; financial time series; fuzzy reasoning; high frequency financial trading; neural networks; pattern recognition; Adaptive systems; Economic forecasting; Expert systems; Finance; Frequency; Fuzzy reasoning; Fuzzy systems; Humans; Neural networks; Pattern recognition; efficient market hypothesis; financial prediction; high frequency trading; neuro-fuzzy inference system},
	Pages = {105--110},
	Title = {Adaptive Neuro Fuzzy Inference Systems for High Frequency Financial Trading and Forecasting},
	Ty = {CONF},
	Year = {2009},
	Year1 = {11-16 Oct. 2009},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ADVCOMP.2009.23}}

@article{2003cond.mat..4469K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2003cond.mat..4469K},
	Author = {{Kondratenko}, V.~V. and {Kuperin}, Y.~A},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {cond-mat/0304469},
	Journal = {eprint arXiv:cond-mat/0304469},
	Keywords = {Condensed Matter - Disordered Systems and Neural Networks, Quantitative Finance - Statistical Finance},
	Month = apr,
	Title = {{Using Recurrent Neural Networks To Forecasting of Forex}},
	Year = 2003}

@article{2015arXiv151207108G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151207108G},
	Archiveprefix = {arXiv},
	Author = {{Gu}, J. and {Wang}, Z. and {Kuen}, J. and {Ma}, L. and {Shahroudy}, A. and {Shuai}, B. and {Liu}, T. and {Wang}, X. and {Wang}, L. and {Wang}, G. and {Cai}, J. and {Chen}, T.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1512.07108},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = dec,
	Primaryclass = {cs.CV},
	Title = {{Recent Advances in Convolutional Neural Networks}},
	Year = 2015}

@article{Niedermeier:2014aa,
	Abstract = {Data driven streaming applications are quite common in modern multimedia and wireless applications, like for example video and audio processing. The main components of these applications are Digital Signal Processing (DSP) algorithms. These algorithms are not extremely complex in terms of their structure and the operations that make up the algorithms are fairly simple (usually binary mathematical operations like addition and multiplication). What makes it challenging to implement and execute these algorithms efficiently is their large degree of fine-grained parallelism and the required throughput. DSP algorithms can usually be described as dataflow graphs with nodes corresponding to operations and edges between the nodes expressing data dependencies. A node fires, i.e. executes, as soon as all required input data has arrived at its input edge(s). To execute DSP algorithms efficiently while maintaining flexibility, coarse-grained reconfigurable arrays (CGRAs) can be used. CGRAs are composed of a set of small, reconfigurable cores, interconnected in e.g. a two dimensional array. Each core by itself is not very powerful, yet the complete array of cores forms an efficient architecture with a high throughput due to its ability to efficiently execute operations in parallel. In this thesis, we present a CGRA targeted at data driven streaming DSP applications that contain a large degree of fine grained parallelism, such as matrix manipulations or filter algorithms. Along with the architecture, also a programming language is presented that can directly describe DSP applications as dataflow graphs which are then automatically mapped and executed on the architecture. In contrast to previously published work on CGRAs, the guiding principle and inspiration for the presented CGRA and its corresponding programming paradigm is the dataflow principle. The result of this work is a completely integrated framework targeted at streaming DSP algorithms, consisting of a CGRA, a programming language and a compiler. The complete system is based on dataflow principles. We conclude that by using an architecture that is based on dataflow principles and a corresponding programming paradigm that can directly express dataflow graphs, DSP algorithms can be implemented in a very intuitive and straightforward manner.},
	Author = {Niedermeier,A.},
	Date = {2014/8/29},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.3990/1.9789036537322},
	Isbn = {978-90-365-3732-2},
	Keywords = {IR-91607; EWI-25011; METIS-304761},
	M3 = {PhD Thesis - Research UT, graduation UT},
	Month = {8},
	N2 = {Data driven streaming applications are quite common in modern multimedia and wireless applications, like for example video and audio processing. The main components of these applications are Digital Signal Processing (DSP) algorithms. These algorithms are not extremely complex in terms of their structure and the operations that make up the algorithms are fairly simple (usually binary mathematical operations like addition and multiplication). What makes it challenging to implement and execute these algorithms efficiently is their large degree of fine-grained parallelism and the required throughput. DSP algorithms can usually be described as dataflow graphs with nodes corresponding to operations and edges between the nodes expressing data dependencies. A node fires, i.e. executes, as soon as all required input data has arrived at its input edge(s). To execute DSP algorithms efficiently while maintaining flexibility, coarse-grained reconfigurable arrays (CGRAs) can be used. CGRAs are composed of a set of small, reconfigurable cores, interconnected in e.g. a two dimensional array. Each core by itself is not very powerful, yet the complete array of cores forms an efficient architecture with a high throughput due to its ability to efficiently execute operations in parallel. In this thesis, we present a CGRA targeted at data driven streaming DSP applications that contain a large degree of fine grained parallelism, such as matrix manipulations or filter algorithms. Along with the architecture, also a programming language is presented that can directly describe DSP applications as dataflow graphs which are then automatically mapped and executed on the architecture. In contrast to previously published work on CGRAs, the guiding principle and inspiration for the presented CGRA and its corresponding programming paradigm is the dataflow principle. The result of this work is a completely integrated framework targeted at streaming DSP algorithms, consisting of a CGRA, a programming language and a compiler. The complete system is based on dataflow principles. We conclude that by using an architecture that is based on dataflow principles and a corresponding programming paradigm that can directly express dataflow graphs, DSP algorithms can be implemented in a very intuitive and straightforward manner.},
	Title = {A fine-grained parallel dataflow-inspired architecture for streaming applications},
	Ty = {THES},
	U2 = {10.3990/1.9789036537322},
	Year = {2014},
	Year1 = {2014/8/29},
	Bdsk-Url-1 = {https://dx.doi.org/10.3990/1.9789036537322}}

@inbook{Smit:2010aa,
	Abstract = {Today the hardware for embedded systems is often specified in VHDL. However, VHDL describes the system at a rather low level, which is cumbersome and may lead to design faults in large real life applications. There is a need of higher level abstraction mechanisms. In the embedded systems group of the University of Twente we are working on systematic and transformational methods to design hardware architectures, both multi core and single core. The main line in this approach is to start with a straightforward (often mathematical) specification of the problem. The next step is to find some adequate transformations on this specification, in particular to find specific optimizations, to be able to distribute the application over different cores. The result of these transformations is then translated into the functional programming language Haskell since Haskell is close to mathematics and such a translation often is straightforward. Besides, the Haskell code is executable, so one immediately has a simulation of the intended system. Next, the resulting Haskell specification is given to a compiler, called C{\"e}aSH (for CAES LAnguage for Synchronous Hardware) which translates the specification into VHDL. The resulting VHDL is synthesizable, so from there on standard VHDL-tooling can be used for synthesis. In this work we primarily focus on streaming applications: i.e. applications that can be modeled as data-flow graphs. At the moment the C{\"e}aSH system is ready in prototype form and in the presentation we will give several examples of how it can be used. In these examples it will be shown that the specification code is clear and concise. Furthermore, it is possible to use powerful abstraction mechanisms, such as polymorphism, higher order functions, pattern matching, lambda abstraction, partial application. These features allow a designer to describe circuits in a more natural and concise way than possible with the language elements found in the traditional hardware description languages. In addition we will give some examples of transformations that are possible in a mathematical specification, and which do not suffer from the problems encountered in, e.g., automatic parallelization of nested for-loops in C-programs.},
	Annote = {eemcs-eprint-19169},
	Author = {Smit,Gerardus Johannes Maria and Kuper,Jan and Baaij,C. P. R.},
	Booktitle = {Dagstuhl Seminar on Dynamically Reconfigurable Architectures},
	Date = {2010/12/14},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.4230/OASIcs.WCET.2010.136},
	Keywords = {IR-75334; METIS-275806; Hardware design; EC Grant Agreement nr.: FP7/248465; Streaming Applications; EWI-19169; mathematical specification},
	M3 = {Conference contribution},
	Month = {12},
	N2 = {Today the hardware for embedded systems is often specified in VHDL. However, VHDL describes the system at a rather low level, which is cumbersome and may lead to design faults in large real life applications. There is a need of higher level abstraction mechanisms. In the embedded systems group of the University of Twente we are working on systematic and transformational methods to design hardware architectures, both multi core and single core. The main line in this approach is to start with a straightforward (often mathematical) specification of the problem. The next step is to find some adequate transformations on this specification, in particular to find specific optimizations, to be able to distribute the application over different cores. The result of these transformations is then translated into the functional programming language Haskell since Haskell is close to mathematics and such a translation often is straightforward. Besides, the Haskell code is executable, so one immediately has a simulation of the intended system. Next, the resulting Haskell specification is given to a compiler, called C{\"e}aSH (for CAES LAnguage for Synchronous Hardware) which translates the specification into VHDL. The resulting VHDL is synthesizable, so from there on standard VHDL-tooling can be used for synthesis. In this work we primarily focus on streaming applications: i.e. applications that can be modeled as data-flow graphs. At the moment the C{\"e}aSH system is ready in prototype form and in the presentation we will give several examples of how it can be used. In these examples it will be shown that the specification code is clear and concise. Furthermore, it is possible to use powerful abstraction mechanisms, such as polymorphism, higher order functions, pattern matching, lambda abstraction, partial application. These features allow a designer to describe circuits in a more natural and concise way than possible with the language elements found in the traditional hardware description languages. In addition we will give some examples of transformations that are possible in a mathematical specification, and which do not suffer from the problems encountered in, e.g., automatic parallelization of nested for-loops in C-programs.},
	Pages = {11},
	Publisher = {Internationales Begegnungs- und Forschungszentrum fur Informatik (IBFI)},
	Title = {A mathematical approach towards hardware design},
	Title1 = {Dagstuhl Seminar Proceedings},
	Ty = {CHAP},
	U2 = {10.4230/OASIcs.WCET.2010.136},
	Year = {2010},
	Year1 = {2010/12/14},
	Bdsk-Url-1 = {https://dx.doi.org/10.4230/OASIcs.WCET.2010.136}}

@article{Wester:2015aa,
	Abstract = {The amount of resources available on reconfigurable logic devices like FPGAs has seen a tremendous growth over the last thirty years. During this period, the amount of programmable resources (CLBs and RAMs) has increased by more than three orders of magnitude. Programming these reconfigurable architectures has been dominated by the hardware description languages VHDL and Verilog. However, it has become generally accepted that these languages do not provide adequate abstraction mechanisms to deliver the design productivity for designing more and more complex applications. To raise the abstraction level, techniques to translate high-level languages to hardware have been developed based on imperative languages like C. Parallelism is achieved by parallelization of for-loops. Whether parallelization of loops is possible, is determined using dependency analysis which is a very hard problem. To mitigate this problem, other abstractions are needed to express parallelism. In this thesis, parallelism is expressed using higher-order functions, an abstraction commonly used in functional programming languages. The main contribution of this thesis is a design methodology based on exploiting regularity of higher-order functions. A mathematical formula, e.g., a DSP algorithm, is first formulated using higher-order functions. Then, transformation rules are applied to these higher-order functions to distribute computations over space and time. Using these transformations, an optimal trade-off can be made between space and time. Finally, hardware is generated using the CLaSH compiler by translating the result of the transformation to VHDL. In this thesis, we derive transformation rules for several higher-order functions and prove that the transformations are meaning-preserving. After transformation, a mathematically equivalent description is derived in which the computations are distributed over space and time. The designer can control the amount of parallelism using a parameter that is introduced by the transformation. Transformation rules for both one-dimensional higher-order functions and two-dimensional higher- order functions have been derived and applied to several case studies: a dot product, a particle filter and stencil computations.},
	Author = {Wester,Rinse},
	Date = {2015/7/3},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.3990/1.9789036538879},
	Isbn = {978-90-365-3887-9},
	Keywords = {Higher-order functions; EWI-26125; IR-96278; METIS-310874; transformations; Hardware design},
	M3 = {PhD Thesis - Research UT, graduation UT},
	Month = {7},
	N2 = {The amount of resources available on reconfigurable logic devices like FPGAs has seen a tremendous growth over the last thirty years. During this period, the amount of programmable resources (CLBs and RAMs) has increased by more than three orders of magnitude. Programming these reconfigurable architectures has been dominated by the hardware description languages VHDL and Verilog. However, it has become generally accepted that these languages do not provide adequate abstraction mechanisms to deliver the design productivity for designing more and more complex applications. To raise the abstraction level, techniques to translate high-level languages to hardware have been developed based on imperative languages like C. Parallelism is achieved by parallelization of for-loops. Whether parallelization of loops is possible, is determined using dependency analysis which is a very hard problem. To mitigate this problem, other abstractions are needed to express parallelism. In this thesis, parallelism is expressed using higher-order functions, an abstraction commonly used in functional programming languages. The main contribution of this thesis is a design methodology based on exploiting regularity of higher-order functions. A mathematical formula, e.g., a DSP algorithm, is first formulated using higher-order functions. Then, transformation rules are applied to these higher-order functions to distribute computations over space and time. Using these transformations, an optimal trade-off can be made between space and time. Finally, hardware is generated using the CLaSH compiler by translating the result of the transformation to VHDL. In this thesis, we derive transformation rules for several higher-order functions and prove that the transformations are meaning-preserving. After transformation, a mathematically equivalent description is derived in which the computations are distributed over space and time. The designer can control the amount of parallelism using a parameter that is introduced by the transformation. Transformation rules for both one-dimensional higher-order functions and two-dimensional higher- order functions have been derived and applied to several case studies: a dot product, a particle filter and stencil computations.},
	Publisher = {Universiteit Twente},
	Title = {A transformation-based approach to hardware design using higher-order functions},
	Ty = {THES},
	U2 = {10.3990/1.9789036538879},
	Year = {2015},
	Year1 = {2015/7/3},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QkS4uL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OLUZpbi9BIERlZXAgTmV1cmFsLU5ldHdvcmsgQmFzZWQgU3RvY2sgVHJhZGluZyBTeXN0ZW0gQmFzZWQgb24gRXZvbHV0aW9uYXJ5IE9wdGltaXplZCBUZWNobmljYWwgQW5hbHlzaXMgUGFyYW1ldGVycy5yaXPSFwsYGVdOUy5kYXRhTxEDCAAAAAADCAACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0EgRGVlcCBOZXVyYWwtTmV0dyNGRkZGRkZGRi5yaXMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAZOTi1GaW4AAgCnLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOk5OLUZpbjpBIERlZXAgTmV1cmFsLU5ldHdvcmsgQmFzZWQgU3RvY2sgVHJhZGluZyBTeXN0ZW0gQmFzZWQgb24gRXZvbHV0aW9uYXJ5IE9wdGltaXplZCBUZWNobmljYWwgQW5hbHlzaXMgUGFyYW1ldGVycy5yaXMAAA4A5gByAEEAIABEAGUAZQBwACAATgBlAHUAcgBhAGwALQBOAGUAdAB3AG8AcgBrACAAQgBhAHMAZQBkACAAUwB0AG8AYwBrACAAVAByAGEAZABpAG4AZwAgAFMAeQBzAHQAZQBtACAAQgBhAHMAZQBkACAAbwBuACAARQB2AG8AbAB1AHQAaQBvAG4AYQByAHkAIABPAHAAdABpAG0AaQB6AGUAZAAgAFQAZQBjAGgAbgBpAGMAYQBsACAAQQBuAGEAbAB5AHMAaQBzACAAUABhAHIAYQBtAGUAdABlAHIAcwAuAHIAaQBzAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIApVVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL0EgRGVlcCBOZXVyYWwtTmV0d29yayBCYXNlZCBTdG9jayBUcmFkaW5nIFN5c3RlbSBCYXNlZCBvbiBFdm9sdXRpb25hcnkgT3B0aW1pemVkIFRlY2huaWNhbCBBbmFseXNpcyBQYXJhbWV0ZXJzLnJpcwAAEwABLwAAFQACAAv//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgEiAScBLwQ7BD0EQgRNBFYEZARoBG8EeAR9BIoEjQSfBKIEpwAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAASp},
	Bdsk-File-2 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8Qfi4uL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OLUZpbi9BZGFwdGl2ZSBOZXVybyBGdXp6eSBJbmZlcmVuY2UgU3lzdGVtcyBmb3IgSGlnaCBGcmVxdWVuY3kgRmluYW5jaWFsIFRyYWRpbmcgYW5kIEZvcmVjYXN0aW5nLnJpc9IXCxgZV05TLmRhdGFPEQK6AAAAAAK6AAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fQWRhcHRpdmUgTmV1cm8gRnV6I0ZGRkZGRkZGLnJpcwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABk5OLUZpbgACAJQvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Tk4tRmluOkFkYXB0aXZlIE5ldXJvIEZ1enp5IEluZmVyZW5jZSBTeXN0ZW1zIGZvciBIaWdoIEZyZXF1ZW5jeSBGaW5hbmNpYWwgVHJhZGluZyBhbmQgRm9yZWNhc3RpbmcucmlzAA4AwABfAEEAZABhAHAAdABpAHYAZQAgAE4AZQB1AHIAbwAgAEYAdQB6AHoAeQAgAEkAbgBmAGUAcgBlAG4AYwBlACAAUwB5AHMAdABlAG0AcwAgAGYAbwByACAASABpAGcAaAAgAEYAcgBlAHEAdQBlAG4AYwB5ACAARgBpAG4AYQBuAGMAaQBhAGwAIABUAHIAYQBkAGkAbgBnACAAYQBuAGQAIABGAG8AcgBlAGMAYQBzAHQAaQBuAGcALgByAGkAcwAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAJJVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OLUZpbi9BZGFwdGl2ZSBOZXVybyBGdXp6eSBJbmZlcmVuY2UgU3lzdGVtcyBmb3IgSGlnaCBGcmVxdWVuY3kgRmluYW5jaWFsIFRyYWRpbmcgYW5kIEZvcmVjYXN0aW5nLnJpcwATAAEvAAAVAAIAC///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAQ8BFAEcA9oD3APhA+wD9QQDBAcEDgQXBBwEKQQsBD4EQQRGAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAABEg=},
	Bdsk-File-3 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QWi4uL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OLUZpbi9Vc2luZyBSZWN1cnJlbnQgTmV1cmFsIE5ldHdvcmtzIFRvIEZvcmVjYXN0aW5nIG9mIEZvcmV4LmJpYtIXCxgZV05TLmRhdGFPEQIqAAAAAAIqAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fVXNpbmcgUmVjdXJyZW50IE5lI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABk5OLUZpbgACAHAvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Tk4tRmluOlVzaW5nIFJlY3VycmVudCBOZXVyYWwgTmV0d29ya3MgVG8gRm9yZWNhc3Rpbmcgb2YgRm9yZXguYmliAA4AeAA7AFUAcwBpAG4AZwAgAFIAZQBjAHUAcgByAGUAbgB0ACAATgBlAHUAcgBhAGwAIABOAGUAdAB3AG8AcgBrAHMAIABUAG8AIABGAG8AcgBlAGMAYQBzAHQAaQBuAGcAIABvAGYAIABGAG8AcgBlAHgALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAG5Vc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OLUZpbi9Vc2luZyBSZWN1cnJlbnQgTmV1cmFsIE5ldHdvcmtzIFRvIEZvcmVjYXN0aW5nIG9mIEZvcmV4LmJpYgATAAEvAAAVAAIAC///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAOsA8AD4AyYDKAMtAzgDQQNPA1MDWgNjA2gDdQN4A4oDjQOSAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAA5Q=},
	Bdsk-Url-1 = {https://dx.doi.org/10.3990/1.9789036538879}}

@inproceedings{Wester:2012aa,
	Author = {R. Wester and C. Baaij and J. Kuper},
	Booktitle = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/FPL.2012.6339258},
	Isbn = {1946-147X},
	Journal = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	Journal1 = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	Keywords = {field programmable gate arrays; functional languages; hardware description languages; logic design; mathematical analysis; particle filtering (numerical methods); program compilers; C\&{\#}x03BB;aSH HDL; DSP application; FPGA; Haskell; adequate abstraction mechanisms; functional hardware description language; higher level abstraction mechanism; higher-order function; mathematical definition; particle filtering; polymorphism; two step hardware design method; Atmospheric measurements; Design methodology; Equations; Hardware; Mathematical model; Particle measurements; Systematics},
	Pages = {181--188},
	Title = {A two step hardware design method using C\&{\#}x03BB;aSH},
	Ty = {CONF},
	Year = {2012},
	Year1 = {29-31 Aug. 2012},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/FPL.2012.6339258}}

@inbook{5ecc1e5c8c7644a9bf1ff0c60033d5faB,
	Abstract = {Functional hardware description languages are a class of hardware description languages that emphasize on the ability to express higher level structural properties, such a parameterization and regularity. Due to such features as higher-order functions and polymorphism, parameterization in functional hardware description languages is more natural than the parameterization support found in the more traditional hardware description languages, like VHDL and Verilog. We de- velop a new functional hardware description language, CλasH, that borrows both the syntax and semantics from the general-purpose functional programming language Haskell.},
	Author = {C.P.R. Baaij},
	Booktitle = {ClasH - From Haskell To Hardware},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {Haskell, CLaSH},
	Month = {12},
	Pages = {101},
	Publisher = {University of Twente},
	Title = {ClasH - From Haskell To Hardware},
	Year = {2009}}

@misc{essay70777,
	Abstract = {ClaSH is a functional hardware description language (HDL) developed at the CAES
group of the University of Twente. ClaSH borrows both the syntax and semantics from the general-purpose functional programming language Haskell, meaning that circuit designers can define their circuits with regular Haskell syntax.

In this thesis, research is done on the co-simulation of ClaSH and traditional HDLs. The Verilog Procedural Interface (VPI), as defined in the IEEE 1364 standard, is used to set-up the communication and to control a Verilog simulator. An implementation is made, as will be described in this thesis, to show the practical feasibility of co-simulation of ClaSH and Verilog.},
	Author = {J.G.J. {Verheij}},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {CLaSH, Haskell, Simulation, HDL},
	Month = {August},
	Title = {Co-simulation between C$\lambda$aSH and traditional HDLs},
	Url = {http://essay.utwente.nl/70777/},
	Year = {2016},
	Bdsk-Url-1 = {http://essay.utwente.nl/70777/}}

@inbook{5ecc1e5c8c7644a9bf1ff0c60033d5fa,
	Abstract = {As embedded systems are becoming increasingly complex, the design process and verification have become very time-consuming. Additionally, specifying hardware manually in a low-level hardware description language like VHDL is usually an error-prone task. In our group, a tool (the ClaSH compiler) was developed to generate fully synthesisable VHDL code from a specification given in the functional programming language Haskell. In this paper, we present a comparison between two implementations of the same design by using ClaSH and hand-written VHDL. The design is a simple dataflow processor. As measures of interest area, performance, power consumption and source lines of code (SLOC) are used. The obtained results indicate that the ClaSH -generated VHDL code as well as the netlist after synthesis and place and route are functionally correct. The placed and routed hand-written VHDL code has also the correct behaviour. Furthermore, a similar performance is achieved. The power consumption is even lower for the ClaSH implementation. The SLOC for ClaSH is considerably smaller and it is possible to specify the design in a much higher level of abstraction compared to VHDL.},
	Author = {A. Niedermeier and Rinse Wester and Rinse Wester and C.P.R. Baaij and Jan Kuper and Smit, {Gerardus Johannes Maria}},
	Booktitle = {Proceedings of the Workshop on PROGram for Research on Embedded Systems and Software (PROGRESS 2010)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Isbn = {978-90-73461-67-3},
	Keywords = {METIS-277454, IR-75095, EWI-18902},
	Month = {11},
	Pages = {216--221},
	Publisher = {Technology Foundation (STW)},
	Title = {Comparing CλaSH and VHDL by implementing a dataflow processor},
	Year = {2010}}

@inproceedings{6339201,
	Author = {B. N. Uchevler and K. Svarstad},
	Booktitle = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/FPL.2012.6339201},
	Issn = {1946-147X},
	Keywords = {circuit complexity;functional languages;hardware description languages;high level synthesis;program compilers;program verification;reconfigurable architectures;CLaSH;RT level VHDL;digital circuit description;digital circuit verification;dynamic reconfigurable system modeling;electronic design complexity;formal verification;functional HDL;high-level Haskell description translation;higher-order functions;parametrization;partial evaluation technique;polymorphism;run-time reconfigurable systems;synthesis tool chain;Communications technology;Consumer electronics;Educational institutions;Field programmable gate arrays;Hardware;Mathematical model;Unified modeling language},
	Month = {Aug},
	Pages = {481-482},
	Title = {Modeling of dynamic reconfigurable systems with Haskell},
	Year = {2012},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QZy4uL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0ZQR0EvQSBQeXRob25pYyBBcHByb2FjaCBmb3IgUmFwaWQgSGFyZHdhcmUgUHJvdG90eXBpbmcgYW5kIEluc3RydW1lbnRhdGlvbi5iaWLSFwsYGVdOUy5kYXRhTxECYgAAAAACYgACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0EgUHl0aG9uaWMgQXBwcm9hYyNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAARGUEdBAAIAfS86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpGUEdBOkEgUHl0aG9uaWMgQXBwcm9hY2ggZm9yIFJhcGlkIEhhcmR3YXJlIFByb3RvdHlwaW5nIGFuZCBJbnN0cnVtZW50YXRpb24uYmliAAAOAJYASgBBACAAUAB5AHQAaABvAG4AaQBjACAAQQBwAHAAcgBvAGEAYwBoACAAZgBvAHIAIABSAGEAcABpAGQAIABIAGEAcgBkAHcAYQByAGUAIABQAHIAbwB0AG8AdAB5AHAAaQBuAGcAIABhAG4AZAAgAEkAbgBzAHQAcgB1AG0AZQBuAHQAYQB0AGkAbwBuAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgB7VXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9GUEdBL0EgUHl0aG9uaWMgQXBwcm9hY2ggZm9yIFJhcGlkIEhhcmR3YXJlIFByb3RvdHlwaW5nIGFuZCBJbnN0cnVtZW50YXRpb24uYmliAAATAAEvAAAVAAIAC///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAPgA/QEFA2sDbQNyA30DhgOUA5gDnwOoA60DugO9A88D0gPXAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAA9k=},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/FPL.2012.6339201}}

@inbook{fff28539e56047c4adca5cc3c9c29cec,
	Abstract = {CλaSH, a functional hardware description language based on Haskell, has several abstraction mechanisms that allow a hardware designer to describe architectures in a short and concise way. In this paper we evaluate CλaSH on a complex DSP application, a Polyphase Filter Bank as it is used in the ASTRON APERTIF project. The Polyphase Filter Bank is implemented in two steps: first in Haskell as being close to a standard mathematical specification, then in CλaSH which is derived from the Haskell formulation by applying only minor changes. We show that the CλaSH formulation can be directly mapped to hardware, thus exploiting the parallelism and concurrency that is present in the original mathematical specification.},
	Author = {Rinse Wester and Dimitrios Sarakiotis and Eric Kooistra and Jan Kuper},
	Booktitle = {Communicating Process Architectures 2012},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Isbn = {978-0-9565409-5-9},
	Keywords = {EWI-22586, EC Grant Agreement nr.: FP7/248465, Specification, METIS-289800, APERTIF Project, CλaSH, IR-82307},
	Month = {8},
	Note = {eemcs-eprint-22586},
	Pages = {53--64},
	Publisher = {Open Channel Publishing},
	Title = {Specification of APERTIF Polyphase Filter Bank in CλaSH},
	Year = {2012}}

@inproceedings{6523639,
	Author = {B. N. Uchevler and K. Svarstad and J. Kuper and C. Baaij},
	Booktitle = {International Symposium on Quality Electronic Design (ISQED)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/ISQED.2013.6523639},
	Issn = {1948-3287},
	Keywords = {field programmable gate arrays;hardware description languages;logic design;CLaSH;FPGA design;RT level;Suzaku-sz410 board;digital circuit verification;dynamic reconfigurable designs;formal verification;functional HDL;functional programming abstractions;high-level Haskell descriptions;high-level descriptions;high-level structures;higher-order functions;partial evaluation implementation technique;run-time reconfigurable systems;synthesizable VHDL;system-level modelling;Consumer electronics;Digital signal processing;Field programmable gate arrays;Finite impulse response filters;Hardware;Software;Unified modeling language;Functional HDL;Partial Evaluation;Run-Time Reconfiguration;Self-Reconfiguration},
	Month = {March},
	Pages = {379-385},
	Title = {System-level modelling of dynamic reconfigurable designs using functional programming abstractions},
	Year = {2013},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ISQED.2013.6523639}}

@inproceedings{Oancea:2012:FSG:2364474.2364484,
	Acmid = {2364484},
	Address = {New York, NY, USA},
	Author = {Oancea, Cosmin E. and Andreetta, Christian and Berthold, Jost and Frisch, Alain and Henglein, Fritz},
	Booktitle = {Proceedings of the 1st ACM SIGPLAN Workshop on Functional High-performance Computing},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/2364474.2364484},
	Isbn = {978-1-4503-1577-7},
	Keywords = {autoparallelization, functional language, memory coalescing, strength reduction, tiling},
	Location = {Copenhagen, Denmark},
	Numpages = {12},
	Pages = {61--72},
	Publisher = {ACM},
	Series = {FHPC '12},
	Title = {Financial Software on GPUs: Between Haskell and Fortran},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2364474.2364484},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2364474.2364484},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2364474.2364484}}

@inproceedings{Funie:2014aa,
	Author = {A. I. Funie and M. Salmon and W. Luk},
	Booktitle = {2014 13th International Conference on Machine Learning and Applications},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/ICMLA.2014.11},
	Journal = {2014 13th International Conference on Machine Learning and Applications},
	Journal1 = {2014 13th International Conference on Machine Learning and Applications},
	Keywords = {economics; field programmable gate arrays; foreign exchange trading; genetic algorithms; particle swarm optimisation; economic value; field programmable gate array technology; financial markets; foreign exchange market data; genetic programming; high frequency trading strategies; hybrid evolutionary algorithm; monitor market stability; particle swarm optimisation; Algorithm design and analysis; Genetics; Noise; Prediction algorithms; Sociology; Statistics; Testing},
	Pages = {29--34},
	Title = {A Hybrid Genetic-Programming Swarm-Optimisation Approach for Examining the Nature and Stability of High Frequency Trading Strategies},
	Ty = {CONF},
	Year = {2014},
	Year1 = {3-6 Dec. 2014},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ICMLA.2014.11}}

@inproceedings{Lockwood:2012aa,
	Author = {J. W. Lockwood and A. Gupte and N. Mehta and M. Blott and T. English and K. Vissers},
	Booktitle = {2012 IEEE 20th Annual Symposium on High-Performance Interconnects},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/HOTI.2012.15},
	Isbn = {1550-4794},
	Journal = {2012 IEEE 20th Annual Symposium on High-Performance Interconnects},
	Journal1 = {2012 IEEE 20th Annual Symposium on High-Performance Interconnects},
	Keywords = {IP networks; electronic engineering computing; electronic trading; field programmable gate arrays; local area networks; memory protocols; microprocessor chips; network interfaces; Ethernet line rate; FPGA IP library; FPGA hardware; HFT platform; I-O interface; alternative hybrid architecture; bit rate 10 Gbit/s; computers software; custom 1U FPGA appliance; electronic trading; financial protocol parser; fixed end-to-end latency; hardware acceleration; high-frequency trading platform; high-performance network adapter; low-latency library; memory interface; pre-built infrastructure; software implementation; time 1 mus; Field programmable gate arrays; Hardware; IP networks; Libraries; Protocols; Registers; Software; Algorithmic; FPGA; HFT; latency; trading},
	Pages = {9--16},
	Title = {A Low-Latency Library in FPGA Hardware for High-Frequency Trading (HFT)},
	Ty = {CONF},
	Year = {2012},
	Year1 = {22-24 Aug. 2012},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/HOTI.2012.15}}

@inproceedings{Zoican:2016aa,
	Author = {S. Zoican and M. Vochin},
	Booktitle = {2016 International Conference on Communications (COMM)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/ICComm.2016.7528289},
	Journal = {2016 International Conference on Communications (COMM)},
	Journal1 = {2016 International Conference on Communications (COMM)},
	Keywords = {financial data processing; computing system; financial market literature; high frequency trading applications; high frequency trading financial applications; high processing speed; high-frequency traders; low latency technology; low network latency; medium cost technology; network architectures; optimal trading speed; Bandwidth; Computer architecture; Computers; Graphics processing units; Instruction sets; Parallel processing; Servers; computer unified device architecture; high frequency trading algorithms; network latency},
	Pages = {139--144},
	Title = {Computing system and network architectures in high frequency trading financial applications},
	Ty = {CONF},
	Year = {2016},
	Year1 = {9-10 June 2016},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ICComm.2016.7528289}}

@inproceedings{Litz:2011:DPE:2088256.2088268,
	Acmid = {2088268},
	Address = {New York, NY, USA},
	Author = {Litz, Heiner and Leber, Christian and Geib, Benjamin},
	Booktitle = {Proceedings of the Fourth Workshop on High Performance Computational Finance},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/2088256.2088268},
	Isbn = {978-1-4503-1108-3},
	Keywords = {DSL, FAST, FIX, FPGA, decoder, domain specific language, high throughput, low latency, stock, trading},
	Location = {Seattle, Washington, USA},
	Numpages = {8},
	Pages = {31--38},
	Publisher = {ACM},
	Series = {WHPCF '11},
	Title = {DSL Programmable Engine for High Frequency Trading Acceleration},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2088256.2088268},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2088256.2088268},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2088256.2088268}}

@inproceedings{Woods:2008aa,
	Author = {N. A. Woods and T. VanCourt},
	Booktitle = {2008 International Conference on Field Programmable Logic and Applications},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/FPL.2008.4629954},
	Isbn = {1946-147X},
	Journal = {2008 International Conference on Field Programmable Logic and Applications},
	Journal1 = {2008 International Conference on Field Programmable Logic and Applications},
	Keywords = {Monte Carlo methods; field programmable gate arrays; financial data processing; FPGA acceleration; finance; multicore processor; pricing simulations; quasiMonte Carlo methods; Acceleration; Computational modeling; Field programmable gate arrays; Finance; Monte Carlo methods; Multicore processing; Pricing; Runtime; Security; Yield estimation},
	Pages = {335--340},
	Title = {FPGA acceleration of quasi-Monte Carlo in finance},
	Ty = {CONF},
	Year = {2008},
	Year1 = {8-10 Sept. 2008},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/FPL.2008.4629954}}

@article{Tian:2010:HQC:1862648.1862656,
	Acmid = {1862656},
	Address = {New York, NY, USA},
	Articleno = {26},
	Author = {Tian, Xiang and Benkrid, Khaled},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/1862648.1862656},
	Issn = {1936-7406},
	Issue_Date = {November 2010},
	Journal = {ACM Trans. Reconfigurable Technol. Syst.},
	Keywords = {CPU, FPGA, GPU, Maxwell, Quasi-Monte Carlo simulations, option pricing},
	Month = nov,
	Number = {4},
	Numpages = {22},
	Pages = {26:1--26:22},
	Publisher = {ACM},
	Title = {High-Performance Quasi-Monte Carlo Financial Simulation: FPGA vs. GPP vs. GPU},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1862648.1862656},
	Volume = {3},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1862648.1862656},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1862648.1862656}}

@inproceedings{Dvorak:2014aa,
	Author = {M. Dvo{\v r}{\'a}k and J. Ko{\v r}enek},
	Booktitle = {17th International Symposium on Design and Diagnostics of Electronic Circuits \& Systems},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/DDECS.2014.6868785},
	Journal = {17th International Symposium on Design and Diagnostics of Electronic Circuits \& Systems},
	Journal1 = {17th International Symposium on Design and Diagnostics of Electronic Circuits \& Systems},
	Keywords = {electronic trading; field programmable gate arrays; logic design; FPGA; QDR SRAM; algorithmic trading; best bid price; best offer price; financial instrument; hardware architecture; hardware market state; high frequency trading; lookup latency-memory utilization trade-off; low latency book handling; low latency trading system; market data processing; storage capacity 144 Mbit; Algorithm design and analysis; Feeds; Field programmable gate arrays; Hardware; Instruments; Memory management},
	Pages = {175--178},
	Title = {Low latency book handling in FPGA for high frequency trading},
	Ty = {CONF},
	Year = {2014},
	Year1 = {23-25 April 2014},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QXy4uL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OL0VxdWlsaWJyYXRlZCBhZGFwdGl2ZSBsZWFybmluZyByYXRlcyBmb3Igbm9uLWNvbnZleCBvcHRpbWl6YXRpb24uYmli0hcLGBlXTlMuZGF0YU8RAkQAAAAAAkQAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9FcXVpbGlicmF0ZWQgYWRhcHQjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAACTk4AAgB1LzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOk5OOkVxdWlsaWJyYXRlZCBhZGFwdGl2ZSBsZWFybmluZyByYXRlcyBmb3Igbm9uLWNvbnZleCBvcHRpbWl6YXRpb24uYmliAAAOAIoARABFAHEAdQBpAGwAaQBiAHIAYQB0AGUAZAAgAGEAZABhAHAAdABpAHYAZQAgAGwAZQBhAHIAbgBpAG4AZwAgAHIAYQB0AGUAcwAgAGYAbwByACAAbgBvAG4ALQBjAG8AbgB2AGUAeAAgAG8AcAB0AGkAbQBpAHoAYQB0AGkAbwBuAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBzVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9OTi9FcXVpbGlicmF0ZWQgYWRhcHRpdmUgbGVhcm5pbmcgcmF0ZXMgZm9yIG5vbi1jb252ZXggb3B0aW1pemF0aW9uLmJpYgAAEwABLwAAFQACAAv//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgDwAPUA/QNFA0cDTANXA2ADbgNyA3kDggOHA5QDlwOpA6wDsQAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAOz},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/DDECS.2014.6868785}}

@article{Thomas:2013aa,
	Author = {D. B. Thomas and W. Luk},
	Booktitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/TVLSI.2012.2228017},
	Isbn = {1063-8210},
	Journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Journal1 = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Keywords = {Gaussian distribution; adders; field programmable gate arrays; graphics processing units; random number generation; shift registers; table lookup; Virtex-5 FPGA; adders; block-memory resources; field-programmable gate array; frequency 1.2 GHz; frequency 400 MHz; graphics processing unit; independent Gaussian samples; logic resources; lookup-tables; multiplierless algorithm; multivariate Gaussian distribution; multivariate Gaussian vectors; multivariate generator; numerical simulation; pair-wise correlations; random number generation; read-only memories; registers; scalar Gaussian generator; uniform distribution; Covariance matrix; Field programmable gate arrays; Generators; Matrix decomposition; Standards; Table lookup; Vectors; Field-programmable gate array (FPGA); Monte Carlo simulation; multivariate samples; random number generation},
	Number = {12},
	Pages = {2193--2205},
	Title = {Multiplierless Algorithm for Multivariate Gaussian Random Number Generation in FPGAs},
	Ty = {JOUR},
	Vo = {21},
	Volume = {21},
	Year = {2013},
	Year1 = {Dec. 2013},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/TVLSI.2012.2228017}}

@article{2017arXiv171105860H-2,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171105860H},
	Archiveprefix = {arXiv},
	Author = {{Hao}, Y.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1711.05860},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Hardware Architecture, Computer Science - Neural and Evolutionary Computing},
	Month = nov,
	Primaryclass = {cs.CV},
	Title = {{A General Neural Network Hardware Architecture on FPGA}},
	Year = 2017}

@article{2017arXiv170206392L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170206392L},
	Archiveprefix = {arXiv},
	Author = {{Li}, Y. and {Liu}, Z. and {Xu}, K. and {Yu}, H. and {Ren}, F.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1702.06392},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, C.3},
	Month = feb,
	Primaryclass = {cs.DC},
	Title = {{A GPU-Outperforming FPGA Accelerator Architecture for Binary Convolutional Neural Networks}},
	Year = 2017}

@article{2017arXiv170808917D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170808917D},
	Archiveprefix = {arXiv},
	Author = {{Ding}, C. and {Liao}, S. and {Wang}, Y. and {Li}, Z. and {Liu}, N. and {Zhuo}, Y. and {Wang}, C. and {Qian}, X. and {Bai}, Y. and {Yuan}, G. and {Ma}, X. and {Zhang}, Y. and {Tang}, J. and {Qiu}, Q. and {Lin}, X. and {Yuan}, B.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1708.08917},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
	Month = aug,
	Primaryclass = {cs.CV},
	Title = {{CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-CirculantWeight Matrices}},
	Year = 2017}

@inproceedings{Zhao:2016aa,
	Author = {Wenlai Zhao and Haohuan Fu and W. Luk and Teng Yu and Shaojun Wang and Bo Feng and Yuchun Ma and Guangwen Yang},
	Booktitle = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/ASAP.2016.7760779},
	Journal = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	Journal1 = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	Keywords = {field programmable gate arrays; floating point arithmetic; neural nets; 32-bit floating-point arithmetic; FPGA-based framework; bandwidth resources; convolutional neural networks; hardware resources; streaming datapath; Bandwidth; Computational modeling; Convolution; Field programmable gate arrays; Neural networks; Runtime; Training},
	Pages = {107--114},
	Title = {F-CNN: An FPGA-based framework for training Convolutional Neural Networks},
	Ty = {CONF},
	Year = {2016},
	Year1 = {6-8 July 2016},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QVi4uL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0ZQR0EvQWNjZWxlcmF0aW5nIExhcmdlLVNjYWxlIEhQQyBBcHBsaWNhdGlvbnMgVXNpbmcgRlBHQXMucmlz0hcLGBlXTlMuZGF0YU8RAhwAAAAAAhwAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9BY2NlbGVyYXRpbmcgTGFyZ2UjRkZGRkZGRkYucmlzAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAERlBHQQACAGwvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RlBHQTpBY2NlbGVyYXRpbmcgTGFyZ2UtU2NhbGUgSFBDIEFwcGxpY2F0aW9ucyBVc2luZyBGUEdBcy5yaXMADgB0ADkAQQBjAGMAZQBsAGUAcgBhAHQAaQBuAGcAIABMAGEAcgBnAGUALQBTAGMAYQBsAGUAIABIAFAAQwAgAEEAcABwAGwAaQBjAGEAdABpAG8AbgBzACAAVQBzAGkAbgBnACAARgBQAEcAQQBzAC4AcgBpAHMADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBqVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9GUEdBL0FjY2VsZXJhdGluZyBMYXJnZS1TY2FsZSBIUEMgQXBwbGljYXRpb25zIFVzaW5nIEZQR0FzLnJpcwATAAEvAAAVAAIAC///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAOcA7AD0AxQDFgMbAyYDLwM9A0EDSANRA1YDYwNmA3gDewOAAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAA4I=},
	Bdsk-File-2 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QMS4uL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0RDR0FOL1dhc3NlcnN0ZWluIEdBTi5iaWLSFwsYGVdOUy5kYXRhTxEBigAAAAABigACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////E1dhc3NlcnN0ZWluIEdBTi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAVEQ0dBTgAAAgBHLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkRDR0FOOldhc3NlcnN0ZWluIEdBTi5iaWIAAA4AKAATAFcAYQBzAHMAZQByAHMAdABlAGkAbgAgAEcAQQBOAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBFVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9EQ0dBTi9XYXNzZXJzdGVpbiBHQU4uYmliAAATAAEvAAAVAAIAC///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAMIAxwDPAl0CXwJkAm8CeAKGAooCkQKaAp8CrAKvAsECxALJAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAss=},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ASAP.2016.7760779}}

@inproceedings{Abdelouahab:2017:WTH:3131885.3131937,
	Acmid = {3131937},
	Address = {New York, NY, USA},
	Author = {Abdelouahab, Kamel and Pelcat, Maxime and Berry, Francois},
	Booktitle = {Proceedings of the 11th International Conference on Distributed Smart Cameras},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/3131885.3131937},
	Isbn = {978-1-4503-5487-5},
	Keywords = {FPGA, TanH, Harware, Acceleration},
	Location = {Stanford, CA, USA},
	Numpages = {3},
	Pages = {199--201},
	Publisher = {ACM},
	Series = {ICDSC 2017},
	Title = {Why TanH is a Hardware Friendly Activation Function for CNNs},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/3131885.3131937},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/3131885.3131937},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/3131885.3131937}}

@article{2017arXiv170304691B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170304691B},
	Archiveprefix = {arXiv},
	Author = {{Borovykh}, A. and {Bohte}, S. and {Oosterlee}, C.~W.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1703.04691},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning},
	Month = mar,
	Primaryclass = {stat.ML},
	Title = {{Conditional Time Series Forecasting with Convolutional Neural Networks}},
	Year = 2017,
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QdC4uL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0NOTi1GUEdBL09wdGltaXppbmcgRlBHQS1iYXNlZCBBY2NlbGVyYXRvciBEZXNpZ24gZm9yIERlZXAgQ29udm9sdXRpb25hbCBOZXVyYWwgTmV0d29ya3MuYmli0hcLGBlXTlMuZGF0YU8RApAAAAAAApAAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9PcHRpbWl6aW5nIEZQR0EtYmEjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAIQ05OLUZQR0EAAgCKLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkNOTi1GUEdBOk9wdGltaXppbmcgRlBHQS1iYXNlZCBBY2NlbGVyYXRvciBEZXNpZ24gZm9yIERlZXAgQ29udm9sdXRpb25hbCBOZXVyYWwgTmV0d29ya3MuYmliAA4AqABTAE8AcAB0AGkAbQBpAHoAaQBuAGcAIABGAFAARwBBAC0AYgBhAHMAZQBkACAAQQBjAGMAZQBsAGUAcgBhAHQAbwByACAARABlAHMAaQBnAG4AIABmAG8AcgAgAEQAZQBlAHAAIABDAG8AbgB2AG8AbAB1AHQAaQBvAG4AYQBsACAATgBlAHUAcgBhAGwAIABOAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAIhVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0NOTi1GUEdBL09wdGltaXppbmcgRlBHQS1iYXNlZCBBY2NlbGVyYXRvciBEZXNpZ24gZm9yIERlZXAgQ29udm9sdXRpb25hbCBOZXVyYWwgTmV0d29ya3MuYmliABMAAS8AABUAAgAL//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4BBQEKARIDpgOoA60DuAPBA88D0wPaA+MD6AP1A/gECgQNBBIAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAEFA==},
	Bdsk-File-2 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8Qei4uL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0NOTi1GUEdBL1BpcGVDTk4tIEFuIE9wZW5DTC1CYXNlZCBPcGVuLVNvdXJjZSBGUEdBIEFjY2VsZXJhdG9yIGZvciBDb252b2x1dGlvbiBOZXVyYWwgTmV0d29ya3MuYmli0hcLGBlXTlMuZGF0YU8RAqgAAAAAAqgAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9QaXBlQ05OLSBBbiBPcGVuQ0wjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAIQ05OLUZQR0EAAgCQLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkNOTi1GUEdBOlBpcGVDTk4tIEFuIE9wZW5DTC1CYXNlZCBPcGVuLVNvdXJjZSBGUEdBIEFjY2VsZXJhdG9yIGZvciBDb252b2x1dGlvbiBOZXVyYWwgTmV0d29ya3MuYmliAA4AtABZAFAAaQBwAGUAQwBOAE4ALQAgAEEAbgAgAE8AcABlAG4AQwBMAC0AQgBhAHMAZQBkACAATwBwAGUAbgAtAFMAbwB1AHIAYwBlACAARgBQAEcAQQAgAEEAYwBjAGUAbABlAHIAYQB0AG8AcgAgAGYAbwByACAAQwBvAG4AdgBvAGwAdQB0AGkAbwBuACAATgBlAHUAcgBhAGwAIABOAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAI5Vc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0NOTi1GUEdBL1BpcGVDTk4tIEFuIE9wZW5DTC1CYXNlZCBPcGVuLVNvdXJjZSBGUEdBIEFjY2VsZXJhdG9yIGZvciBDb252b2x1dGlvbiBOZXVyYWwgTmV0d29ya3MuYmliABMAAS8AABUAAgAL//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4BCwEQARgDxAPGA8sD1gPfA+0D8QP4BAEEBgQTBBYEKAQrBDAAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAEMg==}}

@inproceedings{Ding:2015:DLE:2832415.2832572,
	Acmid = {2832572},
	Author = {Ding, Xiao and Zhang, Yue and Liu, Ting and Duan, Junwen},
	Booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Isbn = {978-1-57735-738-4},
	Keywords = {Deep learning, CNN, NN, S&P},
	Location = {Buenos Aires, Argentina},
	Numpages = {7},
	Pages = {2327--2333},
	Publisher = {AAAI Press},
	Series = {IJCAI'15},
	Title = {Deep Learning for Event-driven Stock Prediction},
	Url = {http://dl.acm.org/citation.cfm?id=2832415.2832572},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2832415.2832572}}

@inproceedings{Chen:2016aa,
	Author = {J. F. Chen and W. L. Chen and C. P. Huang and S. H. Huang and A. P. Chen},
	Booktitle = {2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/CCBD.2016.027},
	Journal = {2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
	Journal1 = {2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
	Keywords = {decision support systems; feature extraction; feedforward neural nets; financial data processing; learning (artificial intelligence); stock markets; time series; FinTech; Taiwan Stock Index Futures; artificial intelligence; deep convolutional neural networks; deep learning; feature extraction; financial markets; financial time-series data analysis; historical datasets; intelligent trading decision support system; multimedia fields; next financial technology generation; numerical features; planar feature representation; time-series data prediction; time-series data processing; trading simulation application; Data models; Feature extraction; Machine learning; Market research; Neural networks; Time series analysis; Training; Deep learning; convolutional neural networks; data visualization; machine learning; trend prediction},
	Pages = {87--92},
	Title = {Financial Time-Series Data Analysis Using Deep Convolutional Neural Networks},
	Ty = {CONF},
	Year = {2016},
	Year1 = {16-18 Nov. 2016},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/CCBD.2016.027}}

@inproceedings{Tsantekidis:2017aa,
	Abstract = {In today's financial markets, where most trades are performed in their entirety by electronic means and the largest fraction of them is completely automated, an opportunity has risen from analyzing this vast amount of transactions. Since all the transactions are recorded in great detail, investors can analyze all the generated data and detect repeated patterns of the price movements. Being able to detect them in advance, allows them to take profitable positions or avoid anomalous events in the financial markets. In this work we proposed a deep learning methodology, based on Convolutional Neural Networks (CNNs), that predicts the price movements of stocks, using as input large-scale, high-frequency time-series derived from the order book of financial exchanges. The dataset that we use contains more than 4 million limit order events and our comparison with other methods, like Multilayer Neural Networks and Support Vector Machines, shows that CNNs are better suited for this kind of task.},
	Author = {A. Tsantekidis and N. Passalis and A. Tefas and J. Kanniainen and M. Gabbouj and A. Iosifidis},
	Booktitle = {2017 IEEE 19th Conference on Business Informatics (CBI)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/CBI.2017.23},
	Journal = {2017 IEEE 19th Conference on Business Informatics (CBI)},
	Journal1 = {2017 IEEE 19th Conference on Business Informatics (CBI)},
	Keywords = {economic forecasting; feedforward neural nets; learning (artificial intelligence); pricing; stock markets; time series; CNN; convolutional neural networks; deep learning methodology; financial exchanges; financial markets; input large-scale high-frequency time-series; limit order book; price movements; stock price forecasting; stock price movement prediction; transaction analysis; Convolution; Data models; Machine learning; Market research; Mathematical model; Neural networks; Support vector machines; Convolutional Neural Networks; Large scale financial data; Limit Orderbook},
	Pages = {7--12},
	Title = {Forecasting Stock Prices from the Limit Order Book Using Convolutional Neural Networks},
	Ty = {CONF},
	Vo = {01},
	Volume = {01},
	Year = {2017},
	Year1 = {24-27 July 2017},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/CBI.2017.23}}

@article{Gunduz:2017aa,
	Author = {Gunduz, Hakan and Yaslan, Yusuf and Cataltepe, Zehra},
	Da = {2017/12/01/},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {https://doi.org/10.1016/j.knosys.2017.09.023},
	Isbn = {0950-7051},
	Journal = {Knowledge-Based Systems},
	Keywords = {Stock market prediction; Deep learning; Borsa Istanbul; Convolutional neural networks; CNN; Feature selection; Feature correlations},
	Number = {Supplement C},
	Pages = {138--148},
	Title = {Intraday prediction of Borsa Istanbul using convolutional neural networks and feature correlations},
	Ty = {JOUR},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950705117304252},
	Volume = {137},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950705117304252},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.knosys.2017.09.023}}

@article{2016arXiv160306995C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160306995C},
	Archiveprefix = {arXiv},
	Author = {{Cui}, Z. and {Chen}, W. and {Chen}, Y.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1603.06995},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = mar,
	Primaryclass = {cs.CV},
	Title = {{Multi-Scale Convolutional Neural Networks for Time Series Classification}},
	Year = 2016}

@article{2017arXiv171105860H,
	Adsnote = {International Journal of Computer Science and Information Technologies (IJCSIT{\textregistered}) is published using an open access publishing model, which makes the full-text of all peer-reviewed papers freely available online with no subscription or registration barriers.},
	Adsurl = {http://ijcsit.com/docs/Volume%207/vol7issue5/ijcsit20160705014.pdf},
	Archiveprefix = {pdf},
	Author = {{Bhandare}, Ashwin, {Bhide}, Maithili, {Gokhale}, Pranav, {Chandavarkar}, Rohan,},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1711.05860},
	Journal = {International Journal of Computer Science and Information Technologies},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	Month = 5,
	Primaryclass = {cs.CV},
	Title = {{Applications of Convolutional Neural Networks}},
	Year = 2016,
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QYi4uL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OLUZpbi9EZWVwIEludmVzdG1lbnQgaW4gRmluYW5jaWFsIE1hcmtldHMgdXNpbmcgRGVlcCBMZWFybmluZyBNb2RlbHMuYmli0hcLGBlXTlMuZGF0YU8RAkoAAAAAAkoAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9EZWVwIEludmVzdG1lbnQgaW4jRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAGTk4tRmluAAIAeC86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpOTi1GaW46RGVlcCBJbnZlc3RtZW50IGluIEZpbmFuY2lhbCBNYXJrZXRzIHVzaW5nIERlZXAgTGVhcm5pbmcgTW9kZWxzLmJpYgAOAIgAQwBEAGUAZQBwACAASQBuAHYAZQBzAHQAbQBlAG4AdAAgAGkAbgAgAEYAaQBuAGEAbgBjAGkAYQBsACAATQBhAHIAawBlAHQAcwAgAHUAcwBpAG4AZwAgAEQAZQBlAHAAIABMAGUAYQByAG4AaQBuAGcAIABNAG8AZABlAGwAcwAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAdlVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL0RlZXAgSW52ZXN0bWVudCBpbiBGaW5hbmNpYWwgTWFya2V0cyB1c2luZyBEZWVwIExlYXJuaW5nIE1vZGVscy5iaWIAEwABLwAAFQACAAv//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgDzAPgBAANOA1ADVQNgA2kDdwN7A4IDiwOQA50DoAOyA7UDugAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAO8}}
